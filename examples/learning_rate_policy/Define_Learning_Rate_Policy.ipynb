{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Learning Rate Policy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is one of the most important hyperparameter to train for you neural network in order to achieve good performance. In the tutorial, you will learn how to specify predefined learning rate policy and customize your own learning rate with FCMP function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, learning_rate_policy are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import swat\n",
    "import sys\n",
    "import dlpy\n",
    "from dlpy.layers import *\n",
    "from dlpy.model import *\n",
    "from dlpy.images import ImageTable\n",
    "from dlpy.sequential import Sequential\n",
    "from dlpy.lr_scheduler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name='your_host_name'\n",
    "port_number='your_port_number'\n",
    "sess = swat.CAS(host_name, port_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network\n",
    "First, Let's build a simple ResNet like model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, size, stride=1, mode='same', act=True):\n",
    "    x = Conv2d(filters, size, size, act='identity', include_bias=False, stride=stride)(x)\n",
    "    x = BN(act='relu' if act else 'identity')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(ip, nf=64):\n",
    "    x = conv_block(ip, nf, 3, 2)\n",
    "    x = conv_block(x, nf, 3, 1, act=False)\n",
    "    return Res()([x, ip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "inp_resnet= Input(3, 112, 112, scale = 1.0 / 255, name='InputLayer_1')\n",
    "x=conv_block(inp_resnet, 64, 9, 1)\n",
    "for i in range(4): x=res_block(x)\n",
    "x=Conv2d(20, 9, 9, act='tanh')(x)\n",
    "x=Pooling(7, 7)(x)\n",
    "output = OutputLayer(n=2)(x)\n",
    "resnet_like_model = Model(sess, inputs = inp_resnet, outputs = output)\n",
    "resnet_like_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Model_kxV6go Pages: 1 -->\r\n",
       "<svg width=\"339pt\" height=\"1781pt\"\r\n",
       " viewBox=\"0.00 0.00 338.50 1781.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1777)\">\r\n",
       "<title>Model_kxV6go</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1777 334.5,-1777 334.5,4 -4,4\"/>\r\n",
       "<!-- InputLayer_1 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>InputLayer_1</title>\r\n",
       "<polygon fill=\"#3288bd\" fill-opacity=\"0.250980\" stroke=\"#3288bd\" points=\"89.5,-1750.5 89.5,-1772.5 315.5,-1772.5 315.5,-1750.5 89.5,-1750.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"202.5\" y=\"-1757.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">112x112x3 InputLayer_1(input)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>Conv2d_1</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"118,-1680.5 118,-1702.5 287,-1702.5 287,-1680.5 118,-1680.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"202.5\" y=\"-1687.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">9x9 Conv2d_1(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- InputLayer_1&#45;&gt;Conv2d_1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>InputLayer_1&#45;&gt;Conv2d_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M202.5,-1750.47C202.5,-1740.62 202.5,-1725.33 202.5,-1712.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"206,-1712.57 202.5,-1702.57 199,-1712.57 206,-1712.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"235.5\" y=\"-1724\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 112 x 112 x 3 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_1 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>BN_1</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"90.5,-1610.5 90.5,-1632.5 314.5,-1632.5 314.5,-1610.5 90.5,-1610.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"202.5\" y=\"-1617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">112x112x64 BN_1(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_1&#45;&gt;BN_1 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>Conv2d_1&#45;&gt;BN_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M202.5,-1680.47C202.5,-1670.62 202.5,-1655.33 202.5,-1642.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"206,-1642.57 202.5,-1632.57 199,-1642.57 206,-1642.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"238\" y=\"-1654\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 112 x 112 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_2 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>Conv2d_2</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"49,-1540.5 49,-1562.5 218,-1562.5 218,-1540.5 49,-1540.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"133.5\" y=\"-1547.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_2(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_1&#45;&gt;Conv2d_2 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>BN_1&#45;&gt;Conv2d_2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M190.717,-1610.46C184.57,-1605.11 176.998,-1598.34 170.5,-1592 163.441,-1585.12 155.952,-1577.24 149.572,-1570.34\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"152.015,-1567.82 142.683,-1562.81 146.849,-1572.55 152.015,-1567.82\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-1584\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 112 x 112 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Res_1 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>Res_1</title>\r\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"105,-1260.5 105,-1282.5 302,-1282.5 302,-1260.5 105,-1260.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"203.5\" y=\"-1267.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">56x56x64 Res_1(residual)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_1&#45;&gt;Res_1 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>BN_1&#45;&gt;Res_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M220.71,-1610.48C228.014,-1605.7 235.99,-1599.4 241.5,-1592 252.626,-1577.06 255.5,-1571.13 255.5,-1552.5 255.5,-1552.5 255.5,-1552.5 255.5,-1340.5 255.5,-1319.88 240.221,-1301.63 226.228,-1289.25\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"228.198,-1286.33 218.262,-1282.66 223.737,-1291.73 228.198,-1286.33\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"291\" y=\"-1444\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 112 x 112 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_2 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>BN_2</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"20,-1470.5 20,-1492.5 227,-1492.5 227,-1470.5 20,-1470.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"123.5\" y=\"-1477.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">56x56x64 BN_2(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_2&#45;&gt;BN_2 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>Conv2d_2&#45;&gt;BN_2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M132.024,-1540.47C130.562,-1530.52 128.282,-1515.01 126.447,-1502.54\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"129.899,-1501.96 124.982,-1492.57 122.974,-1502.98 129.899,-1501.96\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"160\" y=\"-1514\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_3 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>Conv2d_3</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"39,-1400.5 39,-1422.5 208,-1422.5 208,-1400.5 39,-1400.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"123.5\" y=\"-1407.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_3(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_2&#45;&gt;Conv2d_3 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>BN_2&#45;&gt;Conv2d_3</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M123.5,-1470.47C123.5,-1460.62 123.5,-1445.33 123.5,-1432.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"127,-1432.57 123.5,-1422.57 120,-1432.57 127,-1432.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"154\" y=\"-1444\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_3 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>BN_3</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"20,-1330.5 20,-1352.5 227,-1352.5 227,-1330.5 20,-1330.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"123.5\" y=\"-1337.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">56x56x64 BN_3(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_3&#45;&gt;BN_3 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>Conv2d_3&#45;&gt;BN_3</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M123.5,-1400.47C123.5,-1390.62 123.5,-1375.33 123.5,-1362.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"127,-1362.57 123.5,-1352.57 120,-1362.57 127,-1362.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"154\" y=\"-1374\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_3&#45;&gt;Res_1 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>BN_3&#45;&gt;Res_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M135.304,-1330.47C148.066,-1319.62 168.619,-1302.15 183.83,-1289.22\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"186.295,-1291.72 191.647,-1282.57 181.761,-1286.38 186.295,-1291.72\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"199\" y=\"-1304\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_4 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>Conv2d_4</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"67,-1190.5 67,-1212.5 236,-1212.5 236,-1190.5 67,-1190.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"151.5\" y=\"-1197.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_4(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_1&#45;&gt;Conv2d_4 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>Res_1&#45;&gt;Conv2d_4</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M195.827,-1260.47C187.839,-1250.02 175.155,-1233.43 165.398,-1220.67\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"168.059,-1218.39 159.204,-1212.57 162.499,-1222.64 168.059,-1218.39\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-1234\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Res_2 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>Res_2</title>\r\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"119,-910.5 119,-932.5 316,-932.5 316,-910.5 119,-910.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"217.5\" y=\"-917.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">28x28x64 Res_2(residual)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_1&#45;&gt;Res_2 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>Res_1&#45;&gt;Res_2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M224.021,-1260.3C243.395,-1249.05 269.5,-1228.99 269.5,-1202.5 269.5,-1202.5 269.5,-1202.5 269.5,-990.5 269.5,-969.877 254.221,-951.632 240.228,-939.246\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"242.198,-936.334 232.262,-932.657 237.737,-941.728 242.198,-936.334\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"300\" y=\"-1094\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_4 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>BN_4</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"34,-1120.5 34,-1142.5 241,-1142.5 241,-1120.5 34,-1120.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-1127.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">28x28x64 BN_4(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_4&#45;&gt;BN_4 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>Conv2d_4&#45;&gt;BN_4</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M149.434,-1190.47C147.387,-1180.52 144.194,-1165.01 141.626,-1152.54\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"145.019,-1151.66 139.574,-1142.57 138.163,-1153.08 145.019,-1151.66\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"176\" y=\"-1164\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_5 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>Conv2d_5</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"53,-1050.5 53,-1072.5 222,-1072.5 222,-1050.5 53,-1050.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-1057.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_5(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_4&#45;&gt;Conv2d_5 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>BN_4&#45;&gt;Conv2d_5</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M137.5,-1120.47C137.5,-1110.62 137.5,-1095.33 137.5,-1082.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"141,-1082.57 137.5,-1072.57 134,-1082.57 141,-1082.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-1094\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_5 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>BN_5</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"34,-980.5 34,-1002.5 241,-1002.5 241,-980.5 34,-980.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-987.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">28x28x64 BN_5(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_5&#45;&gt;BN_5 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>Conv2d_5&#45;&gt;BN_5</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M137.5,-1050.47C137.5,-1040.62 137.5,-1025.33 137.5,-1012.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"141,-1012.57 137.5,-1002.57 134,-1012.57 141,-1012.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-1024\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_5&#45;&gt;Res_2 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>BN_5&#45;&gt;Res_2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M149.304,-980.466C162.066,-969.619 182.619,-952.149 197.83,-939.22\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"200.295,-941.718 205.647,-932.575 195.761,-936.385 200.295,-941.718\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"213\" y=\"-954\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_6 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>Conv2d_6</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"39,-840.5 39,-862.5 208,-862.5 208,-840.5 39,-840.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"123.5\" y=\"-847.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_6(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_2&#45;&gt;Conv2d_6 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>Res_2&#45;&gt;Conv2d_6</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M196.977,-910.44C187.398,-905.395 176.033,-898.896 166.5,-892 157.453,-885.455 148.23,-877.229 140.658,-869.98\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"142.76,-867.139 133.175,-862.624 137.853,-872.131 142.76,-867.139\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-884\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Res_3 -->\r\n",
       "<g id=\"node18\" class=\"node\"><title>Res_3</title>\r\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"91,-560.5 91,-582.5 288,-582.5 288,-560.5 91,-560.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-567.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">14x14x64 Res_3(residual)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_2&#45;&gt;Res_3 -->\r\n",
       "<g id=\"edge20\" class=\"edge\"><title>Res_2&#45;&gt;Res_3</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M223.556,-910.296C230.666,-897.327 241.5,-874.033 241.5,-852.5 241.5,-852.5 241.5,-852.5 241.5,-640.5 241.5,-619.877 226.221,-601.632 212.228,-589.246\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"214.198,-586.334 204.262,-582.657 209.737,-591.728 214.198,-586.334\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"272\" y=\"-744\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_6 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>BN_6</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"6,-770.5 6,-792.5 213,-792.5 213,-770.5 6,-770.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-777.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">14x14x64 BN_6(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_6&#45;&gt;BN_6 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>Conv2d_6&#45;&gt;BN_6</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M121.434,-840.466C119.387,-830.523 116.194,-815.014 113.626,-802.54\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"117.019,-801.664 111.574,-792.575 110.163,-803.075 117.019,-801.664\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"148\" y=\"-814\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_7 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>Conv2d_7</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"25,-700.5 25,-722.5 194,-722.5 194,-700.5 25,-700.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-707.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_7(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_6&#45;&gt;Conv2d_7 -->\r\n",
       "<g id=\"edge17\" class=\"edge\"><title>BN_6&#45;&gt;Conv2d_7</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M109.5,-770.466C109.5,-760.623 109.5,-745.327 109.5,-732.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"113,-732.575 109.5,-722.575 106,-732.575 113,-732.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"140\" y=\"-744\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_7 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>BN_7</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"6,-630.5 6,-652.5 213,-652.5 213,-630.5 6,-630.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-637.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">14x14x64 BN_7(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_7&#45;&gt;BN_7 -->\r\n",
       "<g id=\"edge18\" class=\"edge\"><title>Conv2d_7&#45;&gt;BN_7</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M109.5,-700.466C109.5,-690.623 109.5,-675.327 109.5,-662.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"113,-662.575 109.5,-652.575 106,-662.575 113,-662.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"140\" y=\"-674\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_7&#45;&gt;Res_3 -->\r\n",
       "<g id=\"edge19\" class=\"edge\"><title>BN_7&#45;&gt;Res_3</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M121.304,-630.466C134.066,-619.619 154.619,-602.149 169.83,-589.22\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"172.295,-591.718 177.647,-582.575 167.761,-586.385 172.295,-591.718\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"185\" y=\"-604\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_8 -->\r\n",
       "<g id=\"node19\" class=\"node\"><title>Conv2d_8</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"19,-490.5 19,-512.5 188,-512.5 188,-490.5 19,-490.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"103.5\" y=\"-497.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_8(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_3&#45;&gt;Conv2d_8 -->\r\n",
       "<g id=\"edge21\" class=\"edge\"><title>Res_3&#45;&gt;Conv2d_8</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M171.85,-560.458C163.314,-555.321 153.082,-548.739 144.5,-542 136.051,-535.365 127.386,-527.245 120.211,-520.095\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"122.592,-517.524 113.091,-512.835 117.594,-522.425 122.592,-517.524\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"175\" y=\"-534\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Res_4 -->\r\n",
       "<g id=\"node23\" class=\"node\"><title>Res_4</title>\r\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"82.5,-210.5 82.5,-232.5 262.5,-232.5 262.5,-210.5 82.5,-210.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-217.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">7x7x64 Res_4(residual)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_3&#45;&gt;Res_4 -->\r\n",
       "<g id=\"edge26\" class=\"edge\"><title>Res_3&#45;&gt;Res_4</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M197.382,-560.06C206.271,-547.225 219.5,-524.436 219.5,-502.5 219.5,-502.5 219.5,-502.5 219.5,-290.5 219.5,-270.534 205.619,-252.186 192.944,-239.594\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"195.052,-236.775 185.343,-232.535 190.288,-241.904 195.052,-236.775\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"250\" y=\"-394\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_8 -->\r\n",
       "<g id=\"node20\" class=\"node\"><title>BN_8</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"0,-420.5 0,-442.5 191,-442.5 191,-420.5 0,-420.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-427.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">7x7x64 BN_8(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_8&#45;&gt;BN_8 -->\r\n",
       "<g id=\"edge22\" class=\"edge\"><title>Conv2d_8&#45;&gt;BN_8</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M102.32,-490.466C101.15,-480.523 99.3252,-465.014 97.8576,-452.54\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"101.33,-452.097 96.6853,-442.575 94.3777,-452.915 101.33,-452.097\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"125.5\" y=\"-464\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_9 -->\r\n",
       "<g id=\"node21\" class=\"node\"><title>Conv2d_9</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"11,-350.5 11,-372.5 180,-372.5 180,-350.5 11,-350.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-357.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_9(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_8&#45;&gt;Conv2d_9 -->\r\n",
       "<g id=\"edge23\" class=\"edge\"><title>BN_8&#45;&gt;Conv2d_9</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M95.5,-420.466C95.5,-410.623 95.5,-395.327 95.5,-382.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"99.0001,-382.575 95.5,-372.575 92.0001,-382.575 99.0001,-382.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"120.5\" y=\"-394\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_9 -->\r\n",
       "<g id=\"node22\" class=\"node\"><title>BN_9</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"0,-280.5 0,-302.5 191,-302.5 191,-280.5 0,-280.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-287.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">7x7x64 BN_9(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_9&#45;&gt;BN_9 -->\r\n",
       "<g id=\"edge24\" class=\"edge\"><title>Conv2d_9&#45;&gt;BN_9</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M95.5,-350.466C95.5,-340.623 95.5,-325.327 95.5,-312.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"99.0001,-312.575 95.5,-302.575 92.0001,-312.575 99.0001,-312.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"120.5\" y=\"-324\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_9&#45;&gt;Res_4 -->\r\n",
       "<g id=\"edge25\" class=\"edge\"><title>BN_9&#45;&gt;Res_4</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M106.862,-280.466C119.145,-269.619 138.927,-252.149 153.568,-239.22\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"155.913,-241.818 161.092,-232.575 151.279,-236.571 155.913,-241.818\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-254\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_10 -->\r\n",
       "<g id=\"node24\" class=\"node\"><title>Conv2d_10</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"84,-140.5 84,-162.5 261,-162.5 261,-140.5 84,-140.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-147.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">9x9 Conv2d_10(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_4&#45;&gt;Conv2d_10 -->\r\n",
       "<g id=\"edge27\" class=\"edge\"><title>Res_4&#45;&gt;Conv2d_10</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M172.5,-210.466C172.5,-200.623 172.5,-185.327 172.5,-172.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"176,-172.575 172.5,-162.575 169,-172.575 176,-172.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-184\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Pooling_1 -->\r\n",
       "<g id=\"node25\" class=\"node\"><title>Pooling_1</title>\r\n",
       "<polygon fill=\"#66c2a5\" fill-opacity=\"0.250980\" stroke=\"#66c2a5\" points=\"94,-70.5 94,-92.5 251,-92.5 251,-70.5 94,-70.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-77.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">7x7 Pooling_1(pool)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_10&#45;&gt;Pooling_1 -->\r\n",
       "<g id=\"edge28\" class=\"edge\"><title>Conv2d_10&#45;&gt;Pooling_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M172.5,-140.466C172.5,-130.623 172.5,-115.327 172.5,-102.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"176,-102.575 172.5,-92.5748 169,-102.575 176,-102.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-114\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 20 </text>\r\n",
       "</g>\r\n",
       "<!-- OutputLayer_1 -->\r\n",
       "<g id=\"node26\" class=\"node\"><title>OutputLayer_1</title>\r\n",
       "<polygon fill=\"#5e4fa2\" fill-opacity=\"0.125490\" stroke=\"#5e4fa2\" points=\"69.5,-0.5 69.5,-22.5 275.5,-22.5 275.5,-0.5 69.5,-0.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">20x2 OutputLayer_1(output)</text>\r\n",
       "</g>\r\n",
       "<!-- Pooling_1&#45;&gt;OutputLayer_1 -->\r\n",
       "<g id=\"edge29\" class=\"edge\"><title>Pooling_1&#45;&gt;OutputLayer_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M172.5,-70.4664C172.5,-60.6231 172.5,-45.327 172.5,-32.9189\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"176,-32.5748 172.5,-22.5748 169,-32.5748 176,-32.5748\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-44\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 1 x 1 x 20 </text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1eaaab07f28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.plot_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='/disk/linux/dlpy/Giraffe_Dolphin'\n",
    "img_path='/cas/DeepLearn/data/Giraffe_Dolphin'\n",
    "my_images = ImageTable.load_files(sess, path=img_path)\n",
    "my_images.resize(112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DLPy have some predefined learning rate policies.\n",
    "1. FixedLR\n",
    "2. StepLR\n",
    "3. MultiStepLR\n",
    "4. PolynomialLR\n",
    "5. ReduceLROnPlateau\n",
    "6. CyclicLR\n",
    "\n",
    "Besides, you can also customize your own learning rate policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, learning_rate_policy are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = StepLR(learning_rate=0.0001, gamma=0.1, step_size=2)\n",
    "solver = MomentumSolver(lr_scheduler=lr_scheduler, clip_grad_max = 100, clip_grad_min = -100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=5, reg_l2=0.0005)\n",
    "gpu = Gpu(devices=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training from scratch.\n",
      "NOTE: Using dlgrd010.unx.sas.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       4.15 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.7826     0.4375      0.299     0.23\n",
      "NOTE:      1    64   0.0001           0.7101      0.375      0.299     0.03\n",
      "NOTE:      2    64   0.0001           0.8505        0.5      0.299     0.03\n",
      "NOTE:      3    64   0.0001            0.849        0.5      0.299     0.03\n",
      "NOTE:      4    64   0.0001           0.6962     0.3594      0.299     0.03\n",
      "NOTE:      5    64   0.0001           0.7589     0.4219      0.299     0.03\n",
      "NOTE:      6    64   0.0001           0.6906     0.3594      0.299     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0        0.0001          0.7626     0.4219     0.41\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001            0.674     0.3438      0.299     0.04\n",
      "NOTE:      1    64   0.0001           0.7429     0.4063      0.299     0.03\n",
      "NOTE:      2    64   0.0001           0.7933     0.4531      0.299     0.03\n",
      "NOTE:      3    64   0.0001           0.8799     0.5313      0.299     0.03\n",
      "NOTE:      4    64   0.0001           0.7724     0.4375      0.299     0.03\n",
      "NOTE:      5    64   0.0001           0.8117     0.4688      0.299     0.03\n",
      "NOTE:      6    64   0.0001           0.7663     0.4375      0.299     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1        0.0001          0.7772     0.4397     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.7578     0.4219      0.299     0.04\n",
      "NOTE:      1    64  0.00001            0.764     0.4375      0.299     0.03\n",
      "NOTE:      2    64  0.00001             0.78     0.4531      0.299     0.03\n",
      "NOTE:      3    64  0.00001           0.7375     0.4063      0.299     0.03\n",
      "NOTE:      4    64  0.00001           0.7509     0.4219      0.299     0.03\n",
      "NOTE:      5    64  0.00001           0.7664     0.4375      0.299     0.03\n",
      "NOTE:      6    64  0.00001           0.7473     0.4219      0.299     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          1E-5          0.7577     0.4286     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.8324        0.5      0.299     0.04\n",
      "NOTE:      1    64  0.00001           0.6966      0.375      0.299     0.03\n",
      "NOTE:      2    64  0.00001           0.7794     0.4531      0.299     0.03\n",
      "NOTE:      3    64  0.00001           0.7979     0.4688      0.299     0.03\n",
      "NOTE:      4    64  0.00001           0.6792     0.3594      0.299     0.03\n",
      "NOTE:      5    64  0.00001           0.8266        0.5      0.299     0.03\n",
      "NOTE:      6    64  0.00001           0.7463     0.4219      0.299     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3          1E-5          0.7655     0.4397     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.7612     0.4375      0.299     0.04\n",
      "NOTE:      1    64     1E-6           0.7126     0.3906      0.299     0.03\n",
      "NOTE:      2    64     1E-6           0.7251     0.4063      0.299     0.03\n",
      "NOTE:      3    64     1E-6           0.8398     0.5156      0.299     0.03\n",
      "NOTE:      4    64     1E-6           0.8218        0.5      0.299     0.03\n",
      "NOTE:      5    64     1E-6           0.6463     0.3281      0.299     0.03\n",
      "NOTE:      6    64     1E-6           0.7261     0.4063      0.299     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4          1E-6          0.7476     0.4263     0.21\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       1.24 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_kxv6go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.762578</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.298993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.777216</td>\n",
       "      <td>0.439732</td>\n",
       "      <td>0.298990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.757693</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.298986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.765475</td>\n",
       "      <td>0.439732</td>\n",
       "      <td>0.298984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.747556</td>\n",
       "      <td>0.426339</td>\n",
       "      <td>0.298983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>Model_kxV6go_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_kxV6go_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 5.82s</span> &#183; <span class=\"cas-user\">user 3.5s</span> &#183; <span class=\"cas-sys\">sys 1.61s</span> &#183; <span class=\"cas-memory\">mem 130MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_kxv6go\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "    Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0      1      0.000100  0.762578  0.421875  0.298993\n",
       " 1      2      0.000100  0.777216  0.439732  0.298990\n",
       " 2      3      0.000010  0.757693  0.428571  0.298986\n",
       " 3      4      0.000010  0.765475  0.439732  0.298984\n",
       " 4      5      0.000001  0.747556  0.426339  0.298983\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  Model_kxV6go_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_kxV6go_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 5.82s, user: 3.5s, sys: 1.61s, mem: 130mb"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, n_threads=4, record_seed=13309, optimizer=optimizer,\n",
    "                      gpu=gpu, log_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, learning_rate_policy are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = CyclicLR(conn=sess, data=my_images, max_lr=0.01, batch_size=1, factor=2,\n",
    "                        learning_rate=0.0001)\n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        clip_grad_max = 100, clip_grad_min = -100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=50, reg_l2=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE: Using dlgrd010.unx.sas.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.01 (s).\n",
      "NOTE:  Initializing each layer cost       1.82 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.7269     0.4063      0.299     0.21\n",
      "NOTE:      1    64 0.000112           0.6308     0.3125      0.299     0.03\n",
      "NOTE:      2    64 0.000124           0.7607     0.4375      0.299     0.03\n",
      "NOTE:      3    64 0.000136           0.8379     0.5156      0.299     0.03\n",
      "NOTE:      4    64 0.000148           0.8043     0.4844      0.299     0.03\n",
      "NOTE:      5    64 0.000161           0.8352     0.5156      0.299     0.03\n",
      "NOTE:      6    64 0.000173           0.8976     0.5781      0.299     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0        0.0002          0.7848     0.4643     0.37\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.8184        0.5      0.299     0.04\n",
      "NOTE:      1    64 0.005062           0.7385     0.4219      0.299     0.03\n",
      "NOTE:      2    64 0.005074           0.7016     0.3906      0.299     0.03\n",
      "NOTE:      3    64 0.005086           0.7893        0.5      0.299     0.03\n",
      "NOTE:      4    64 0.005098           0.7821     0.5156      0.299     0.03\n",
      "NOTE:      5    64 0.005111           0.6961     0.4375     0.2989     0.03\n",
      "NOTE:      6    64 0.005123           0.6242     0.3281     0.2989     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1        0.0051          0.7357      0.442     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.6645     0.4219     0.2989     0.04\n",
      "NOTE:      1    64 0.009988           0.6609     0.4219     0.2989     0.03\n",
      "NOTE:      2    64 0.009976           0.6489     0.4531     0.2989     0.03\n",
      "NOTE:      3    64 0.009964           0.6335     0.4219     0.2989     0.04\n",
      "NOTE:      4    64 0.009952           0.6188     0.1094     0.2988     0.03\n",
      "NOTE:      5    64 0.009939           0.5831     0.2188     0.2988     0.03\n",
      "NOTE:      6    64 0.009927           0.5459      0.125     0.2988     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2        0.0099          0.6222     0.3103     0.22\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    48  0.00505           0.5519     0.3333     0.2987     0.03\n",
      "NOTE:      1    64 0.005038           0.4919     0.2031     0.2987     0.03\n",
      "NOTE:      2    64 0.005026           0.4586     0.1875     0.2987     0.03\n",
      "NOTE:      3    64 0.005014           0.4059     0.1406     0.2987     0.03\n",
      "NOTE:      4    64 0.005002           0.3688      0.125     0.2986     0.03\n",
      "NOTE:      5    64 0.004989           0.3143    0.01563     0.2986     0.03\n",
      "NOTE:      6    64 0.004977            0.279      0.125     0.2986     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3         0.005          0.4048     0.1551     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.2758      0.125     0.2986     0.04\n",
      "NOTE:      1    64 0.000112           0.2591      0.125     0.2986     0.03\n",
      "NOTE:      2    64 0.000124           0.2074    0.07813     0.2985     0.03\n",
      "NOTE:      3    64 0.000136           0.2219    0.07813     0.2985     0.03\n",
      "NOTE:      4    64 0.000148           0.1879    0.07813     0.2985     0.03\n",
      "NOTE:      5    64 0.000161           0.2226     0.1094     0.2985     0.03\n",
      "NOTE:      6    64 0.000173           0.2081     0.1094     0.2985     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4        0.0002          0.2261     0.1004     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505            0.159    0.04688     0.2985     0.04\n",
      "NOTE:      1    64 0.005062           0.1221    0.03125     0.2985     0.03\n",
      "NOTE:      2    64 0.005074             0.23    0.09375     0.2985     0.03\n",
      "NOTE:      3    64 0.005086           0.1712    0.09375     0.2985     0.03\n",
      "NOTE:      4    64 0.005098           0.2034    0.07813     0.2985     0.03\n",
      "NOTE:      5    64 0.005111           0.1566    0.04688     0.2985     0.03\n",
      "NOTE:      6    64 0.005123           0.1008          0     0.2985     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5        0.0051          0.1633     0.0558     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.1123    0.03125     0.2985     0.04\n",
      "NOTE:      1    64 0.009988           0.1104    0.04688     0.2985     0.03\n",
      "NOTE:      2    64 0.009976          0.07311          0     0.2984     0.03\n",
      "NOTE:      3    64 0.009964          0.05157          0     0.2984     0.03\n",
      "NOTE:      4    64 0.009952          0.07386    0.01563     0.2984     0.03\n",
      "NOTE:      5    64 0.009939           0.1163    0.04688     0.2984     0.03\n",
      "NOTE:      6    64 0.009927           0.1243     0.0625     0.2984     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6        0.0099         0.09456    0.02902     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.05306    0.01563     0.2983     0.04\n",
      "NOTE:      1    64 0.005038          0.08838    0.03125     0.2983     0.03\n",
      "NOTE:      2    64 0.005026          0.07067    0.04688     0.2983     0.03\n",
      "NOTE:      3    64 0.005014          0.05722    0.01563     0.2983     0.03\n",
      "NOTE:      4    64 0.005002          0.04893    0.01563     0.2982     0.03\n",
      "NOTE:      5    64 0.004989          0.03654    0.01563     0.2982     0.03\n",
      "NOTE:      6    64 0.004977          0.04488    0.01563     0.2982     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.005          0.0571    0.02232     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.05466    0.01563     0.2982     0.04\n",
      "NOTE:      1    64 0.000112          0.03158          0     0.2982     0.03\n",
      "NOTE:      2    64 0.000124          0.04775    0.01563     0.2981     0.03\n",
      "NOTE:      3    64 0.000136           0.1304     0.0625     0.2981     0.03\n",
      "NOTE:      4    64 0.000148          0.03787    0.01563     0.2981     0.03\n",
      "NOTE:      5    64 0.000161          0.08936    0.01563     0.2981     0.03\n",
      "NOTE:      6    64 0.000173          0.01836          0     0.2981     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8        0.0002         0.05857    0.01786     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.02235          0     0.2981     0.04\n",
      "NOTE:      1    64 0.005062          0.04407    0.01563      0.298     0.03\n",
      "NOTE:      2    64 0.005074          0.05489    0.03125      0.298     0.03\n",
      "NOTE:      3    64 0.005086          0.02804    0.01563      0.298     0.03\n",
      "NOTE:      4    64 0.005098          0.01437          0      0.298     0.03\n",
      "NOTE:      5    64 0.005111          0.07118    0.01563      0.298     0.03\n",
      "NOTE:      6    64 0.005123          0.02967    0.01563      0.298     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9        0.0051          0.0378    0.01339     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.05038    0.04688      0.298     0.04\n",
      "NOTE:      1    64 0.009988          0.04606    0.03125     0.2979     0.03\n",
      "NOTE:      2    64 0.009976          0.01751          0     0.2979     0.03\n",
      "NOTE:      3    64 0.009964          0.05166    0.01563     0.2979     0.03\n",
      "NOTE:      4    64 0.009952           0.1122    0.03125     0.2979     0.03\n",
      "NOTE:      5    64 0.009939          0.02261          0     0.2978     0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      6    64 0.009927           0.0735    0.03125     0.2978     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10       0.0099         0.05342    0.02232     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.03814          0     0.2978     0.04\n",
      "NOTE:      1    64 0.005038           0.2051    0.07813     0.2977     0.03\n",
      "NOTE:      2    64 0.005026          0.05964    0.03125     0.2977     0.03\n",
      "NOTE:      3    64 0.005014          0.03539    0.01563     0.2977     0.03\n",
      "NOTE:      4    64 0.005002          0.01237          0     0.2976     0.03\n",
      "NOTE:      5    64 0.004989          0.07719    0.01563     0.2976     0.03\n",
      "NOTE:      6    64 0.004977          0.02887          0     0.2976     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.005         0.06525    0.02009     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.02391          0     0.2976     0.04\n",
      "NOTE:      1    64 0.000112          0.03481          0     0.2975     0.03\n",
      "NOTE:      2    64 0.000124           0.0459    0.01563     0.2975     0.03\n",
      "NOTE:      3    64 0.000136          0.02508          0     0.2975     0.03\n",
      "NOTE:      4    64 0.000148          0.05194    0.01563     0.2975     0.03\n",
      "NOTE:      5    64 0.000161          0.03711          0     0.2974     0.03\n",
      "NOTE:      6    64 0.000173          0.04122          0     0.2974     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12       0.0002         0.03714   0.004464     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.06248    0.01563     0.2974     0.04\n",
      "NOTE:      1    64 0.005062          0.06464    0.01563     0.2974     0.03\n",
      "NOTE:      2    64 0.005074          0.06313          0     0.2974     0.03\n",
      "NOTE:      3    64 0.005086          0.06037    0.01563     0.2974     0.03\n",
      "NOTE:      4    64 0.005098          0.03894    0.01563     0.2973     0.03\n",
      "NOTE:      5    64 0.005111           0.0606    0.01563     0.2973     0.03\n",
      "NOTE:      6    64 0.005123          0.05268    0.01563     0.2973     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13       0.0051         0.05755    0.01339     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.07218          0     0.2973     0.04\n",
      "NOTE:      1    64 0.009988          0.01134          0     0.2973     0.03\n",
      "NOTE:      2    64 0.009976          0.01882          0     0.2973     0.03\n",
      "NOTE:      3    64 0.009964          0.01873          0     0.2972     0.03\n",
      "NOTE:      4    64 0.009952          0.03661          0     0.2972     0.03\n",
      "NOTE:      5    64 0.009939           0.0195          0     0.2972     0.03\n",
      "NOTE:      6    64 0.009927          0.02211          0     0.2971     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14       0.0099         0.02847          0     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.02891    0.01563     0.2971     0.04\n",
      "NOTE:      1    64 0.005038          0.02795          0     0.2971     0.03\n",
      "NOTE:      2    64 0.005026          0.03296    0.01563      0.297     0.03\n",
      "NOTE:      3    64 0.005014           0.0215          0      0.297     0.03\n",
      "NOTE:      4    64 0.005002           0.1152    0.04688      0.297     0.03\n",
      "NOTE:      5    64 0.004989          0.01362          0     0.2969     0.03\n",
      "NOTE:      6    64 0.004977          0.02216          0     0.2969     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15        0.005         0.03748    0.01116     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.008196          0     0.2969     0.04\n",
      "NOTE:      1    64 0.000112          0.05676    0.01563     0.2968     0.03\n",
      "NOTE:      2    64 0.000124          0.02879          0     0.2968     0.03\n",
      "NOTE:      3    64 0.000136          0.02176          0     0.2968     0.03\n",
      "NOTE:      4    64 0.000148          0.04937    0.01563     0.2968     0.03\n",
      "NOTE:      5    64 0.000161          0.03924    0.01563     0.2967     0.03\n",
      "NOTE:      6    64 0.000173          0.02041          0     0.2967     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16       0.0002         0.03208   0.006696     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.008644          0     0.2967     0.04\n",
      "NOTE:      1    64 0.005062          0.01052          0     0.2967     0.03\n",
      "NOTE:      2    64 0.005074          0.01913          0     0.2967     0.03\n",
      "NOTE:      3    64 0.005086          0.02365          0     0.2967     0.03\n",
      "NOTE:      4    64 0.005098          0.01464          0     0.2966     0.03\n",
      "NOTE:      5    64 0.005111          0.02138    0.01563     0.2966     0.03\n",
      "NOTE:      6    64 0.005123          0.01818          0     0.2966     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0051         0.01659   0.002232     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.006528          0     0.2966     0.04\n",
      "NOTE:      1    64 0.009988          0.02516    0.01563     0.2966     0.03\n",
      "NOTE:      2    64 0.009976         0.009664          0     0.2965     0.03\n",
      "NOTE:      3    64 0.009964          0.01196          0     0.2965     0.03\n",
      "NOTE:      4    64 0.009952         0.007646          0     0.2965     0.03\n",
      "NOTE:      5    64 0.009939         0.007499          0     0.2964     0.03\n",
      "NOTE:      6    64 0.009927         0.006455          0     0.2964     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18       0.0099          0.0107   0.002232     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.005308          0     0.2964     0.04\n",
      "NOTE:      1    64 0.005038         0.007897          0     0.2963     0.03\n",
      "NOTE:      2    64 0.005026          0.02942          0     0.2963     0.03\n",
      "NOTE:      3    64 0.005014          0.02502    0.01563     0.2962     0.03\n",
      "NOTE:      4    64 0.005002          0.01104          0     0.2962     0.03\n",
      "NOTE:      5    64 0.004989           0.0335    0.03125     0.2962     0.03\n",
      "NOTE:      6    64 0.004977         0.007739          0     0.2961     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19        0.005         0.01713   0.006696     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.03589    0.03125     0.2961     0.04\n",
      "NOTE:      1    64 0.000112          0.03681          0     0.2961     0.03\n",
      "NOTE:      2    64 0.000124          0.05549    0.01563      0.296     0.03\n",
      "NOTE:      3    64 0.000136          0.01978          0      0.296     0.03\n",
      "NOTE:      4    64 0.000148         0.006814          0      0.296     0.03\n",
      "NOTE:      5    64 0.000161         0.005307          0      0.296     0.03\n",
      "NOTE:      6    64 0.000173          0.01574          0      0.296     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20       0.0002         0.02512   0.006696     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.005172          0     0.2959     0.04\n",
      "NOTE:      1    64 0.005062          0.01146          0     0.2959     0.03\n",
      "NOTE:      2    64 0.005074          0.00768          0     0.2959     0.03\n",
      "NOTE:      3    64 0.005086          0.05468    0.01563     0.2959     0.03\n",
      "NOTE:      4    64 0.005098         0.008223          0     0.2959     0.03\n",
      "NOTE:      5    64 0.005111          0.02274          0     0.2958     0.03\n",
      "NOTE:      6    64 0.005123          0.01308          0     0.2958     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21       0.0051         0.01758   0.002232     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.008436          0     0.2958     0.04\n",
      "NOTE:      1    64 0.009988         0.009175          0     0.2958     0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      2    64 0.009976         0.005868          0     0.2958     0.03\n",
      "NOTE:      3    64 0.009964          0.01203          0     0.2957     0.03\n",
      "NOTE:      4    64 0.009952          0.04636    0.01563     0.2957     0.03\n",
      "NOTE:      5    64 0.009939         0.004891          0     0.2957     0.03\n",
      "NOTE:      6    64 0.009927           0.0151          0     0.2956     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22       0.0099         0.01455   0.002232     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.005732          0     0.2956     0.04\n",
      "NOTE:      1    64 0.005038          0.00567          0     0.2955     0.03\n",
      "NOTE:      2    64 0.005026         0.005115          0     0.2955     0.03\n",
      "NOTE:      3    64 0.005014          0.01128          0     0.2955     0.03\n",
      "NOTE:      4    64 0.005002          0.01025          0     0.2954     0.03\n",
      "NOTE:      5    64 0.004989          0.02597          0     0.2954     0.03\n",
      "NOTE:      6    64 0.004977          0.00556          0     0.2954     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23        0.005         0.00994          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.01901          0     0.2953     0.04\n",
      "NOTE:      1    64 0.000112         0.004266          0     0.2953     0.03\n",
      "NOTE:      2    64 0.000124         0.005501          0     0.2953     0.03\n",
      "NOTE:      3    64 0.000136          0.01156          0     0.2952     0.03\n",
      "NOTE:      4    64 0.000148          0.01062          0     0.2952     0.03\n",
      "NOTE:      5    64 0.000161         0.005225          0     0.2952     0.03\n",
      "NOTE:      6    64 0.000173           0.1495    0.07813     0.2952     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24       0.0002         0.02938    0.01116     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.005105          0     0.2952     0.04\n",
      "NOTE:      1    64 0.005062          0.02233    0.01563     0.2951     0.03\n",
      "NOTE:      2    64 0.005074         0.006854          0     0.2951     0.03\n",
      "NOTE:      3    64 0.005086         0.004809          0     0.2951     0.03\n",
      "NOTE:      4    64 0.005098          0.00399          0     0.2951     0.03\n",
      "NOTE:      5    64 0.005111         0.009778          0     0.2951     0.03\n",
      "NOTE:      6    64 0.005123         0.005519          0      0.295     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25       0.0051        0.008341   0.002232     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.01065          0      0.295     0.04\n",
      "NOTE:      1    64 0.009988         0.007647          0      0.295     0.03\n",
      "NOTE:      2    64 0.009976          0.02018          0      0.295     0.03\n",
      "NOTE:      3    64 0.009964         0.005407          0     0.2949     0.03\n",
      "NOTE:      4    64 0.009952         0.005138          0     0.2949     0.03\n",
      "NOTE:      5    64 0.009939         0.004582          0     0.2949     0.03\n",
      "NOTE:      6    64 0.009927         0.007144          0     0.2948     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26       0.0099        0.008679          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.0147          0     0.2948     0.04\n",
      "NOTE:      1    64 0.005038         0.005788          0     0.2948     0.03\n",
      "NOTE:      2    64 0.005026          0.02691    0.01563     0.2947     0.03\n",
      "NOTE:      3    64 0.005014          0.00887          0     0.2947     0.03\n",
      "NOTE:      4    64 0.005002          0.02065    0.01563     0.2946     0.03\n",
      "NOTE:      5    64 0.004989          0.01626    0.01563     0.2946     0.03\n",
      "NOTE:      6    64 0.004977         0.003719          0     0.2946     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27        0.005         0.01384   0.006696     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.04542    0.01563     0.2945     0.04\n",
      "NOTE:      1    64 0.000112          0.01579    0.01563     0.2945     0.03\n",
      "NOTE:      2    64 0.000124          0.01717    0.01563     0.2945     0.03\n",
      "NOTE:      3    64 0.000136         0.004261          0     0.2945     0.03\n",
      "NOTE:      4    64 0.000148         0.006565          0     0.2944     0.03\n",
      "NOTE:      5    64 0.000161         0.007987          0     0.2944     0.03\n",
      "NOTE:      6    64 0.000173          0.03695    0.01563     0.2944     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28       0.0002         0.01916   0.008929     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.03192    0.03125     0.2944     0.04\n",
      "NOTE:      1    64 0.005062          0.01755    0.01563     0.2944     0.03\n",
      "NOTE:      2    64 0.005074          0.04321    0.03125     0.2943     0.03\n",
      "NOTE:      3    64 0.005086          0.04558    0.01563     0.2943     0.03\n",
      "NOTE:      4    64 0.005098         0.005144          0     0.2943     0.03\n",
      "NOTE:      5    64 0.005111          0.03804    0.01563     0.2943     0.03\n",
      "NOTE:      6    64 0.005123         0.004259          0     0.2943     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29       0.0051         0.02653    0.01563     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.004938          0     0.2942     0.04\n",
      "NOTE:      1    64 0.009988         0.003683          0     0.2942     0.03\n",
      "NOTE:      2    64 0.009976         0.003301          0     0.2942     0.03\n",
      "NOTE:      3    64 0.009964         0.003737          0     0.2942     0.03\n",
      "NOTE:      4    64 0.009952         0.007101          0     0.2941     0.03\n",
      "NOTE:      5    64 0.009939         0.005167          0     0.2941     0.03\n",
      "NOTE:      6    64 0.009927          0.00703          0     0.2941     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30       0.0099        0.004994          0     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.002906          0      0.294     0.04\n",
      "NOTE:      1    64 0.005038          0.01506    0.01563      0.294     0.03\n",
      "NOTE:      2    64 0.005026         0.005177          0     0.2939     0.03\n",
      "NOTE:      3    64 0.005014          0.03515    0.01563     0.2939     0.03\n",
      "NOTE:      4    64 0.005002         0.004597          0     0.2939     0.03\n",
      "NOTE:      5    64 0.004989         0.007077          0     0.2938     0.03\n",
      "NOTE:      6    64 0.004977         0.007033          0     0.2938     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31        0.005           0.011   0.004464     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.003082          0     0.2938     0.04\n",
      "NOTE:      1    64 0.000112         0.004333          0     0.2937     0.03\n",
      "NOTE:      2    64 0.000124          0.01474          0     0.2937     0.03\n",
      "NOTE:      3    64 0.000136          0.04344    0.01563     0.2937     0.03\n",
      "NOTE:      4    64 0.000148         0.005131          0     0.2936     0.03\n",
      "NOTE:      5    64 0.000161          0.04649    0.01563     0.2936     0.03\n",
      "NOTE:      6    64 0.000173           0.0475    0.03125     0.2936     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32       0.0002         0.02353   0.008929     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.003624          0     0.2936     0.04\n",
      "NOTE:      1    64 0.005062         0.004214          0     0.2936     0.03\n",
      "NOTE:      2    64 0.005074         0.007189          0     0.2936     0.03\n",
      "NOTE:      3    64 0.005086         0.003604          0     0.2935     0.03\n",
      "NOTE:      4    64 0.005098           0.0029          0     0.2935     0.03\n",
      "NOTE:      5    64 0.005111         0.003468          0     0.2935     0.03\n",
      "NOTE:      6    64 0.005123          0.00327          0     0.2935     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33       0.0051        0.004038          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.00654          0     0.2934     0.04\n",
      "NOTE:      1    64 0.009988         0.003544          0     0.2934     0.03\n",
      "NOTE:      2    64 0.009976         0.003257          0     0.2934     0.03\n",
      "NOTE:      3    64 0.009964         0.003073          0     0.2934     0.03\n",
      "NOTE:      4    64 0.009952         0.003468          0     0.2933     0.03\n",
      "NOTE:      5    64 0.009939          0.01493          0     0.2933     0.03\n",
      "NOTE:      6    64 0.009927         0.002718          0     0.2933     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34       0.0099        0.005361          0     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.002968          0     0.2932     0.04\n",
      "NOTE:      1    64 0.005038          0.01584          0     0.2932     0.03\n",
      "NOTE:      2    64 0.005026          0.05529    0.01563     0.2931     0.03\n",
      "NOTE:      3    64 0.005014          0.04482    0.01563     0.2931     0.03\n",
      "NOTE:      4    64 0.005002          0.01433          0     0.2931     0.03\n",
      "NOTE:      5    64 0.004989         0.004957          0      0.293     0.03\n",
      "NOTE:      6    64 0.004977          0.03435    0.01563      0.293     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35        0.005         0.02465   0.006696     0.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.004186          0      0.293     0.04\n",
      "NOTE:      1    64 0.000112         0.007472          0     0.2929     0.03\n",
      "NOTE:      2    64 0.000124         0.006696          0     0.2929     0.03\n",
      "NOTE:      3    64 0.000136         0.004035          0     0.2929     0.03\n",
      "NOTE:      4    64 0.000148         0.004431          0     0.2929     0.03\n",
      "NOTE:      5    64 0.000161          0.00338          0     0.2928     0.03\n",
      "NOTE:      6    64 0.000173           0.0456    0.01563     0.2928     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36       0.0002         0.01083   0.002232     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.003138          0     0.2928     0.04\n",
      "NOTE:      1    64 0.005062         0.003102          0     0.2928     0.03\n",
      "NOTE:      2    64 0.005074         0.003856          0     0.2928     0.03\n",
      "NOTE:      3    64 0.005086         0.009509          0     0.2927     0.03\n",
      "NOTE:      4    64 0.005098          0.00698          0     0.2927     0.03\n",
      "NOTE:      5    64 0.005111         0.004305          0     0.2927     0.03\n",
      "NOTE:      6    64 0.005123         0.006719          0     0.2927     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37       0.0051        0.005373          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.003936          0     0.2927     0.04\n",
      "NOTE:      1    64 0.009988         0.006077          0     0.2926     0.03\n",
      "NOTE:      2    64 0.009976            0.004          0     0.2926     0.03\n",
      "NOTE:      3    64 0.009964         0.005785          0     0.2926     0.03\n",
      "NOTE:      4    64 0.009952         0.006928          0     0.2926     0.03\n",
      "NOTE:      5    64 0.009939         0.006253          0     0.2925     0.03\n",
      "NOTE:      6    64 0.009927         0.003796          0     0.2925     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38       0.0099        0.005253          0     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.006572          0     0.2924     0.04\n",
      "NOTE:      1    64 0.005038          0.04336    0.01563     0.2924     0.03\n",
      "NOTE:      2    64 0.005026         0.005364          0     0.2924     0.03\n",
      "NOTE:      3    64 0.005014         0.005048          0     0.2923     0.03\n",
      "NOTE:      4    64 0.005002         0.004149          0     0.2923     0.03\n",
      "NOTE:      5    64 0.004989         0.006842          0     0.2923     0.03\n",
      "NOTE:      6    64 0.004977           0.0241          0     0.2922     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39        0.005         0.01363   0.002232     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.002852          0     0.2922     0.04\n",
      "NOTE:      1    64 0.000112         0.003158          0     0.2922     0.03\n",
      "NOTE:      2    64 0.000124          0.00241          0     0.2921     0.03\n",
      "NOTE:      3    64 0.000136          0.04268    0.01563     0.2921     0.03\n",
      "NOTE:      4    64 0.000148          0.02368          0     0.2921     0.03\n",
      "NOTE:      5    64 0.000161         0.003467          0     0.2921     0.03\n",
      "NOTE:      6    64 0.000173         0.003729          0      0.292     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40       0.0002         0.01171   0.002232     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.002817          0      0.292     0.04\n",
      "NOTE:      1    64 0.005062         0.006356          0      0.292     0.03\n",
      "NOTE:      2    64 0.005074         0.003136          0      0.292     0.03\n",
      "NOTE:      3    64 0.005086         0.004836          0      0.292     0.03\n",
      "NOTE:      4    64 0.005098         0.002976          0     0.2919     0.03\n",
      "NOTE:      5    64 0.005111          0.01299          0     0.2919     0.03\n",
      "NOTE:      6    64 0.005123         0.008537          0     0.2919     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41       0.0051         0.00595          0     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002959          0     0.2919     0.04\n",
      "NOTE:      1    64 0.009988         0.007249          0     0.2919     0.03\n",
      "NOTE:      2    64 0.009976         0.002696          0     0.2918     0.03\n",
      "NOTE:      3    64 0.009964         0.006493          0     0.2918     0.03\n",
      "NOTE:      4    64 0.009952          0.00303          0     0.2918     0.03\n",
      "NOTE:      5    64 0.009939         0.008393          0     0.2917     0.03\n",
      "NOTE:      6    64 0.009927         0.002692          0     0.2917     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42       0.0099        0.004787          0     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.003209          0     0.2917     0.04\n",
      "NOTE:      1    64 0.005038          0.00586          0     0.2916     0.03\n",
      "NOTE:      2    64 0.005026         0.002558          0     0.2916     0.03\n",
      "NOTE:      3    64 0.005014         0.005483          0     0.2915     0.03\n",
      "NOTE:      4    64 0.005002         0.003173          0     0.2915     0.03\n",
      "NOTE:      5    64 0.004989          0.02915          0     0.2915     0.03\n",
      "NOTE:      6    64 0.004977         0.002938          0     0.2914     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43        0.005        0.007482          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.005532          0     0.2914     0.04\n",
      "NOTE:      1    64 0.000112         0.003628          0     0.2914     0.03\n",
      "NOTE:      2    64 0.000124         0.003024          0     0.2913     0.03\n",
      "NOTE:      3    64 0.000136          0.00698          0     0.2913     0.03\n",
      "NOTE:      4    64 0.000148         0.005194          0     0.2913     0.03\n",
      "NOTE:      5    64 0.000161         0.003558          0     0.2913     0.03\n",
      "NOTE:      6    64 0.000173         0.002525          0     0.2912     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44       0.0002        0.004349          0     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.008322          0     0.2912     0.04\n",
      "NOTE:      1    64 0.005062         0.002792          0     0.2912     0.03\n",
      "NOTE:      2    64 0.005074          0.00282          0     0.2912     0.03\n",
      "NOTE:      3    64 0.005086         0.002537          0     0.2912     0.03\n",
      "NOTE:      4    64 0.005098         0.002297          0     0.2911     0.03\n",
      "NOTE:      5    64 0.005111         0.003034          0     0.2911     0.03\n",
      "NOTE:      6    64 0.005123         0.003458          0     0.2911     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45       0.0051        0.003609          0     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002682          0     0.2911     0.04\n",
      "NOTE:      1    64 0.009988         0.002061          0     0.2911     0.03\n",
      "NOTE:      2    64 0.009976         0.004226          0      0.291     0.03\n",
      "NOTE:      3    64 0.009964         0.002418          0      0.291     0.03\n",
      "NOTE:      4    64 0.009952          0.08254    0.03125      0.291     0.03\n",
      "NOTE:      5    64 0.009939         0.007474          0     0.2909     0.03\n",
      "NOTE:      6    64 0.009927          0.00305          0     0.2909     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46       0.0099         0.01492   0.004464     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.00534          0     0.2909     0.04\n",
      "NOTE:      1    64 0.005038         0.002716          0     0.2908     0.03\n",
      "NOTE:      2    64 0.005026         0.008558          0     0.2908     0.03\n",
      "NOTE:      3    64 0.005014         0.002226          0     0.2907     0.03\n",
      "NOTE:      4    64 0.005002          0.00258          0     0.2907     0.03\n",
      "NOTE:      5    64 0.004989         0.004865          0     0.2907     0.03\n",
      "NOTE:      6    64 0.004977         0.003744          0     0.2906     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47        0.005         0.00429          0     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.002747          0     0.2906     0.04\n",
      "NOTE:      1    64 0.000112         0.005929          0     0.2906     0.03\n",
      "NOTE:      2    64 0.000124         0.002363          0     0.2905     0.03\n",
      "NOTE:      3    64 0.000136         0.002449          0     0.2905     0.03\n",
      "NOTE:      4    64 0.000148         0.005204          0     0.2905     0.03\n",
      "NOTE:      5    64 0.000161         0.005807          0     0.2905     0.03\n",
      "NOTE:      6    64 0.000173            0.039    0.01563     0.2904     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48       0.0002        0.009071   0.002232     0.20\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.005204          0     0.2904     0.04\n",
      "NOTE:      1    64 0.005062         0.007805          0     0.2904     0.03\n",
      "NOTE:      2    64 0.005074         0.002822          0     0.2904     0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      3    64 0.005086         0.002917          0     0.2904     0.03\n",
      "NOTE:      4    64 0.005098          0.03893    0.01563     0.2903     0.03\n",
      "NOTE:      5    64 0.005111         0.002394          0     0.2903     0.03\n",
      "NOTE:      6    64 0.005123         0.002772          0     0.2903     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49       0.0051        0.008979   0.002232     0.21\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      10.45 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_kxv6go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.784783</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.298982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.735732</td>\n",
       "      <td>0.441964</td>\n",
       "      <td>0.298962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.622230</td>\n",
       "      <td>0.310268</td>\n",
       "      <td>0.298847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.404797</td>\n",
       "      <td>0.155093</td>\n",
       "      <td>0.298660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.226127</td>\n",
       "      <td>0.100446</td>\n",
       "      <td>0.298545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.163292</td>\n",
       "      <td>0.055804</td>\n",
       "      <td>0.298504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.094560</td>\n",
       "      <td>0.029018</td>\n",
       "      <td>0.298424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.057097</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.298267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.058569</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.298117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.037798</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.298020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.053421</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.297887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.065249</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>0.297672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.037139</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.297479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.057548</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.297361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.028470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.037476</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.296995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.296784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>23</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.296650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.010702</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.296493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.017131</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.296249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.025118</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.296023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.295881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>28</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.295718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.029376</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.295241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>31</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.295097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.294686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>34</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.019163</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.294460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>35</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.026529</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.294316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>36</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.004994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>37</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.010999</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.293905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>38</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.023530</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.293676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>39</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>40</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.005361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>41</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.024651</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.293112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>42</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.010829</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.292885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>43</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.005373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>44</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>45</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.013634</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.292333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>46</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.292103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>47</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>48</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>49</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>51</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>52</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.014921</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>53</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>54</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.290514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>55</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.290367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>Model_kxV6go_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_kxV6go_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 12.7s</span> &#183; <span class=\"cas-user\">user 20.9s</span> &#183; <span class=\"cas-sys\">sys 4.51s</span> &#183; <span class=\"cas-memory\">mem 130MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_kxv6go\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0       6      0.000173  0.784783  0.464286  0.298982\n",
       " 1       7      0.005123  0.735732  0.441964  0.298962\n",
       " 2       8      0.009927  0.622230  0.310268  0.298847\n",
       " 3       9      0.004977  0.404797  0.155093  0.298660\n",
       " 4      10      0.000173  0.226127  0.100446  0.298545\n",
       " 5      11      0.005123  0.163292  0.055804  0.298504\n",
       " 6      12      0.009927  0.094560  0.029018  0.298424\n",
       " 7      13      0.004977  0.057097  0.022321  0.298267\n",
       " 8      14      0.000173  0.058569  0.017857  0.298117\n",
       " 9      15      0.005123  0.037798  0.013393  0.298020\n",
       " 10     16      0.009927  0.053421  0.022321  0.297887\n",
       " 11     17      0.004977  0.065249  0.020089  0.297672\n",
       " 12     18      0.000173  0.037139  0.004464  0.297479\n",
       " 13     19      0.005123  0.057548  0.013393  0.297361\n",
       " 14     20      0.009927  0.028470  0.000000  0.297220\n",
       " 15     21      0.004977  0.037476  0.011161  0.296995\n",
       " 16     22      0.000173  0.032076  0.006696  0.296784\n",
       " 17     23      0.005123  0.016591  0.002232  0.296650\n",
       " 18     24      0.009927  0.010702  0.002232  0.296493\n",
       " 19     25      0.004977  0.017131  0.006696  0.296249\n",
       " 20     26      0.000173  0.025118  0.006696  0.296023\n",
       " 21     27      0.005123  0.017577  0.002232  0.295881\n",
       " 22     28      0.009927  0.014552  0.002232  0.295718\n",
       " 23     29      0.004977  0.009940  0.000000  0.295470\n",
       " 24     30      0.000173  0.029376  0.011161  0.295241\n",
       " 25     31      0.005123  0.008341  0.002232  0.295097\n",
       " 26     32      0.009927  0.008679  0.000000  0.294933\n",
       " 27     33      0.004977  0.013842  0.006696  0.294686\n",
       " 28     34      0.000173  0.019163  0.008929  0.294460\n",
       " 29     35      0.005123  0.026529  0.015625  0.294316\n",
       " 30     36      0.009927  0.004994  0.000000  0.294153\n",
       " 31     37      0.004977  0.010999  0.004464  0.293905\n",
       " 32     38      0.000173  0.023530  0.008929  0.293676\n",
       " 33     39      0.005123  0.004038  0.000000  0.293530\n",
       " 34     40      0.009927  0.005361  0.000000  0.293364\n",
       " 35     41      0.004977  0.024651  0.006696  0.293112\n",
       " 36     42      0.000173  0.010829  0.002232  0.292885\n",
       " 37     43      0.005123  0.005373  0.000000  0.292744\n",
       " 38     44      0.009927  0.005253  0.000000  0.292581\n",
       " 39     45      0.004977  0.013634  0.002232  0.292333\n",
       " 40     46      0.000173  0.011710  0.002232  0.292103\n",
       " 41     47      0.005123  0.005950  0.000000  0.291958\n",
       " 42     48      0.009927  0.004787  0.000000  0.291793\n",
       " 43     49      0.004977  0.007482  0.000000  0.291543\n",
       " 44     50      0.000173  0.004349  0.000000  0.291312\n",
       " 45     51      0.005123  0.003609  0.000000  0.291167\n",
       " 46     52      0.009927  0.014921  0.004464  0.291000\n",
       " 47     53      0.004977  0.004290  0.000000  0.290747\n",
       " 48     54      0.000173  0.009071  0.002232  0.290514\n",
       " 49     55      0.005123  0.008979  0.002232  0.290367\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  Model_kxV6go_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_kxV6go_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 12.7s, user: 20.9s, sys: 4.51s, mem: 130mb"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, n_threads=4, record_seed=13309, optimizer=optimizer,\n",
    "                      gpu=gpu, log_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Learning Rate on Plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, learning_rate_policy are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(conn=sess, cool_down_iters=2, gamma=0.1, learning_rate=0.01, patience=3)\n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        clip_grad_max = 100, clip_grad_min = -100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=50, reg_l2=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE: Using dlgrd010.unx.sas.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.01 (s).\n",
      "NOTE:  Initializing each layer cost       1.89 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002237          0     0.2903     0.20\n",
      "NOTE:      1    64     0.01         0.002271          0     0.2903     0.03\n",
      "NOTE:      2    64     0.01         0.008064          0     0.2903     0.03\n",
      "NOTE:      3    64     0.01          0.00273          0     0.2903     0.03\n",
      "NOTE:      4    64     0.01         0.002567          0     0.2902     0.03\n",
      "NOTE:      5    64     0.01         0.005389          0     0.2902     0.03\n",
      "NOTE:      6    64     0.01         0.005606          0     0.2902     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0          0.01        0.004123          0     0.37\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002273          0     0.2902     0.04\n",
      "NOTE:      1    64     0.01         0.002147          0     0.2901     0.03\n",
      "NOTE:      2    64     0.01         0.005319          0     0.2901     0.03\n",
      "NOTE:      3    64     0.01         0.002071          0     0.2901     0.03\n",
      "NOTE:      4    64     0.01         0.002505          0       0.29     0.03\n",
      "NOTE:      5    64     0.01         0.005632          0       0.29     0.03\n",
      "NOTE:      6    64     0.01         0.002097          0     0.2899     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1          0.01        0.003149          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.003056          0     0.2899     0.04\n",
      "NOTE:      1    64     0.01           0.0045          0     0.2898     0.03\n",
      "NOTE:      2    64     0.01         0.002132          0     0.2898     0.03\n",
      "NOTE:      3    64     0.01         0.007237          0     0.2897     0.03\n",
      "NOTE:      4    64     0.01          0.03911    0.01563     0.2897     0.03\n",
      "NOTE:      5    64     0.01         0.002911          0     0.2897     0.03\n",
      "NOTE:      6    64     0.01         0.002688          0     0.2896     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          0.01        0.008805   0.002232     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.006297          0     0.2896     0.04\n",
      "NOTE:      1    64     0.01         0.004562          0     0.2895     0.03\n",
      "NOTE:      2    64     0.01         0.002148          0     0.2895     0.03\n",
      "NOTE:      3    64     0.01         0.003157          0     0.2894     0.03\n",
      "NOTE:      4    64     0.01         0.001868          0     0.2893     0.03\n",
      "NOTE:      5    64     0.01         0.002231          0     0.2893     0.03\n",
      "NOTE:      6    64     0.01         0.004979          0     0.2892     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3          0.01        0.003606          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.001914          0     0.2892     0.04\n",
      "NOTE:      1    64     0.01          0.03818    0.01563     0.2891     0.03\n",
      "NOTE:      2    64     0.01           0.0023          0     0.2891     0.03\n",
      "NOTE:      3    64     0.01         0.001745          0      0.289     0.03\n",
      "NOTE:      4    64     0.01         0.005007          0      0.289     0.03\n",
      "NOTE:      5    64     0.01          0.01822          0     0.2889     0.03\n",
      "NOTE:      6    64     0.01          0.03845    0.01563     0.2889     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4          0.01         0.01512   0.004464     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.001833          0     0.2888     0.04\n",
      "NOTE:      1    64     0.01          0.03995    0.01563     0.2888     0.03\n",
      "NOTE:      2    64     0.01          0.00282          0     0.2887     0.03\n",
      "NOTE:      3    64     0.01         0.001903          0     0.2886     0.03\n",
      "NOTE:      4    64     0.01          0.06824    0.04688     0.2886     0.03\n",
      "NOTE:      5    64     0.01         0.001984          0     0.2885     0.03\n",
      "NOTE:      6    64     0.01           0.0385    0.01563     0.2885     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5          0.01         0.02218    0.01116     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.02831    0.01563     0.2884     0.04\n",
      "NOTE:      1    64    0.001          0.02547    0.01563     0.2884     0.03\n",
      "NOTE:      2    64    0.001         0.004446          0     0.2883     0.03\n",
      "NOTE:      3    64    0.001         0.002574          0     0.2883     0.03\n",
      "NOTE:      4    64    0.001         0.005073          0     0.2883     0.03\n",
      "NOTE:      5    64    0.001         0.008479          0     0.2882     0.03\n",
      "NOTE:      6    64    0.001           0.0254    0.01563     0.2882     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6         0.001         0.01425   0.006696     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.03712    0.03125     0.2882     0.04\n",
      "NOTE:      1    64    0.001          0.04246    0.04688     0.2881     0.03\n",
      "NOTE:      2    64    0.001          0.02426    0.01563     0.2881     0.03\n",
      "NOTE:      3    64    0.001          0.09758    0.03125     0.2881     0.03\n",
      "NOTE:      4    64    0.001           0.1236    0.07813     0.2881     0.03\n",
      "NOTE:      5    64    0.001         0.002633          0     0.2881     0.03\n",
      "NOTE:      6    64    0.001         0.009078          0      0.288     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.001          0.0481    0.02902     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.01317          0      0.288     0.04\n",
      "NOTE:      1    64    0.001         0.002877          0      0.288     0.03\n",
      "NOTE:      2    64    0.001          0.02545    0.01563      0.288     0.03\n",
      "NOTE:      3    64    0.001          0.00258          0      0.288     0.03\n",
      "NOTE:      4    64    0.001         0.003265          0      0.288     0.03\n",
      "NOTE:      5    64    0.001          0.02203    0.01563      0.288     0.03\n",
      "NOTE:      6    64    0.001         0.003423          0      0.288     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8         0.001          0.0104   0.004464     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.003998          0     0.2879     0.04\n",
      "NOTE:      1    64    0.001          0.01565          0     0.2879     0.03\n",
      "NOTE:      2    64    0.001          0.02209    0.01563     0.2879     0.03\n",
      "NOTE:      3    64    0.001          0.02882    0.01563     0.2879     0.03\n",
      "NOTE:      4    64    0.001          0.02326    0.01563     0.2879     0.03\n",
      "NOTE:      5    64    0.001         0.004409          0     0.2879     0.03\n",
      "NOTE:      6    64    0.001          0.02646    0.01563     0.2879     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9         0.001         0.01781   0.008929     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.003895          0     0.2879     0.04\n",
      "NOTE:      1    64    0.001         0.004747          0     0.2879     0.03\n",
      "NOTE:      2    64    0.001          0.01858    0.01563     0.2879     0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      3    64    0.001         0.005542          0     0.2879     0.03\n",
      "NOTE:      4    64    0.001         0.002907          0     0.2879     0.03\n",
      "NOTE:      5    64    0.001          0.01954          0     0.2879     0.03\n",
      "NOTE:      6    64    0.001         0.004364          0     0.2878     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10        0.001        0.008511   0.002232     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.01278          0     0.2878     0.04\n",
      "NOTE:      1    64    0.001         0.006091          0     0.2878     0.03\n",
      "NOTE:      2    64    0.001         0.002881          0     0.2878     0.03\n",
      "NOTE:      3    64    0.001         0.006642          0     0.2878     0.03\n",
      "NOTE:      4    64    0.001         0.004963          0     0.2878     0.03\n",
      "NOTE:      5    64    0.001         0.001961          0     0.2878     0.03\n",
      "NOTE:      6    64    0.001         0.002244          0     0.2878     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.001        0.005366          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.004972          0     0.2878     0.04\n",
      "NOTE:      1    64   0.0001         0.004402          0     0.2878     0.03\n",
      "NOTE:      2    64   0.0001          0.01052          0     0.2878     0.03\n",
      "NOTE:      3    64   0.0001         0.001982          0     0.2878     0.03\n",
      "NOTE:      4    64   0.0001          0.01211          0     0.2878     0.03\n",
      "NOTE:      5    64   0.0001         0.002577          0     0.2878     0.03\n",
      "NOTE:      6    64   0.0001         0.002312          0     0.2878     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12       0.0001        0.005553          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.004001          0     0.2878     0.04\n",
      "NOTE:      1    64   0.0001         0.006022          0     0.2878     0.03\n",
      "NOTE:      2    64   0.0001         0.002072          0     0.2878     0.03\n",
      "NOTE:      3    64   0.0001         0.003226          0     0.2878     0.03\n",
      "NOTE:      4    64   0.0001           0.0041          0     0.2878     0.03\n",
      "NOTE:      5    64   0.0001         0.002038          0     0.2878     0.03\n",
      "NOTE:      6    64   0.0001         0.005054          0     0.2878     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13       0.0001        0.003788          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.004548          0     0.2878     0.04\n",
      "NOTE:      1    64   0.0001         0.003259          0     0.2878     0.03\n",
      "NOTE:      2    64   0.0001         0.002676          0     0.2877     0.03\n",
      "NOTE:      3    64   0.0001         0.004201          0     0.2877     0.03\n",
      "NOTE:      4    64   0.0001          0.00468          0     0.2877     0.03\n",
      "NOTE:      5    64   0.0001         0.002376          0     0.2877     0.03\n",
      "NOTE:      6    64   0.0001         0.002259          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14       0.0001        0.003429          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.004617          0     0.2877     0.04\n",
      "NOTE:      1    64   0.0001          0.01189          0     0.2877     0.03\n",
      "NOTE:      2    64   0.0001         0.003459          0     0.2877     0.03\n",
      "NOTE:      3    64   0.0001         0.005439          0     0.2877     0.03\n",
      "NOTE:      4    64   0.0001         0.005353          0     0.2877     0.03\n",
      "NOTE:      5    64   0.0001         0.002322          0     0.2877     0.03\n",
      "NOTE:      6    64   0.0001         0.002114          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15       0.0001        0.005027          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.002258          0     0.2877     0.04\n",
      "NOTE:      1    64   0.0001         0.002492          0     0.2877     0.03\n",
      "NOTE:      2    64   0.0001          0.01031          0     0.2877     0.03\n",
      "NOTE:      3    64   0.0001         0.005031          0     0.2877     0.03\n",
      "NOTE:      4    64   0.0001         0.002412          0     0.2877     0.03\n",
      "NOTE:      5    64   0.0001         0.002202          0     0.2877     0.03\n",
      "NOTE:      6    64   0.0001         0.003762          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16       0.0001        0.004067          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.006351          0     0.2877     0.04\n",
      "NOTE:      1    64   0.0001         0.003091          0     0.2877     0.03\n",
      "NOTE:      2    64   0.0001         0.002457          0     0.2877     0.03\n",
      "NOTE:      3    64   0.0001          0.01099          0     0.2877     0.03\n",
      "NOTE:      4    64   0.0001         0.004079          0     0.2877     0.03\n",
      "NOTE:      5    64   0.0001         0.002551          0     0.2877     0.03\n",
      "NOTE:      6    64   0.0001          0.01099          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0001        0.005786          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002513          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001          0.01429          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.007761          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.002192          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.005768          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.005055          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.002549          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18         1E-5        0.005732          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.004163          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001         0.005075          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001          0.03704          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.007293          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.005752          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.002125          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.005796          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19         1E-5        0.009606          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.001932          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001          0.01466          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.008091          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.006957          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001          0.00414          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001          0.01042          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001          0.00246          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20         1E-5        0.006952          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.01065          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001         0.004775          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.003176          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.005007          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.002752          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001           0.0147          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.005866          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21         1E-5        0.006704          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002148          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001         0.001958          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.004138          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.002314          0     0.2877     0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      4    64  0.00001         0.001761          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.003726          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.004667          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22         1E-5        0.002959          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.005775          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001          0.00218          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001          0.01042          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.002223          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.002377          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.002146          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001          0.01082          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23         1E-5        0.005134          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.02937          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001         0.002291          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001          0.00206          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.002849          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.003085          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.002678          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.006775          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24         1E-5        0.007016          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002693          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001         0.004783          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.004449          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001          0.04376          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.005786          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.004981          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001          0.01097          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25         1E-5         0.01106          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002217          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001         0.002604          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.003927          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.002782          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.002373          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.002304          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.004222          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26         1E-5        0.002919          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002831          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001          0.01106          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.007038          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001          0.00231          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.002398          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.006219          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.002713          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27         1E-5        0.004939          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.001944          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001          0.01272          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.001808          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.002824          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.001903          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.004191          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.002039          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28         1E-5        0.003919          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.004206          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001         0.002192          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.004333          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.002161          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.001974          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.005751          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.004192          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29         1E-5        0.003544          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.01223          0     0.2877     0.04\n",
      "NOTE:      1    64  0.00001           0.0132          0     0.2877     0.03\n",
      "NOTE:      2    64  0.00001         0.002662          0     0.2877     0.03\n",
      "NOTE:      3    64  0.00001         0.002132          0     0.2877     0.03\n",
      "NOTE:      4    64  0.00001         0.005222          0     0.2877     0.03\n",
      "NOTE:      5    64  0.00001         0.004085          0     0.2877     0.03\n",
      "NOTE:      6    64  0.00001         0.004577          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30         1E-5        0.006301          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002422          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-6         0.002062          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-6         0.003142          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-6          0.00393          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-6         0.009669          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-6         0.004483          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-6         0.004223          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31         1E-6        0.004276          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002018          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-6          0.01035          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-6         0.001773          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-6         0.002649          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-6         0.002155          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-6          0.01882          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-6         0.002375          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32         1E-6        0.005734          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.003389          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-6         0.002611          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-6         0.002142          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-6         0.002529          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-6         0.002175          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-6         0.004558          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-6         0.002101          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33         1E-6        0.002786          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002298          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-6         0.002598          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-6         0.004834          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-6          0.00358          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-6         0.002998          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-6         0.003658          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-6         0.002056          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34         1E-6        0.003146          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002479          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-6         0.007694          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-6          0.01218          0     0.2877     0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      3    64     1E-6         0.002246          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-6          0.01033          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-6          0.02149          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-6           0.0106          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35         1E-6        0.009574          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6          0.00212          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-6          0.01046          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-6          0.01077          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-6         0.002109          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-6         0.001985          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-6         0.002272          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-6         0.002654          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36         1E-6        0.004625          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6          0.00268          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-6          0.01098          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-6         0.004261          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-6         0.002333          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-6         0.002008          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-6          0.01896          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-6          0.01131          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37         1E-6        0.007505          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.004796          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-7          0.00258          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-7          0.00334          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-7         0.002379          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-7         0.002436          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-7         0.006159          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-7         0.003925          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38         1E-7        0.003659          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.004761          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-7         0.002655          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-7         0.004353          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-7         0.005239          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-7         0.002402          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-7         0.002426          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-7         0.002932          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39         1E-7        0.003538          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.00225          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-7         0.004128          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-7         0.006737          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-7         0.001945          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-7         0.002618          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-7         0.003921          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-7         0.002532          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40         1E-7        0.003447          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.002215          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-7          0.01531          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-7         0.002308          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-7         0.002781          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-7         0.002276          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-7          0.00492          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-7         0.004329          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41         1E-7        0.004877          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.002249          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-7         0.002593          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-7         0.002135          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-7             0.01          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-7          0.00538          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-7         0.002226          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-7         0.002687          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42         1E-7        0.003896          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.002748          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-7          0.01087          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-7         0.002392          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-7         0.001993          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-7         0.003866          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-7         0.003203          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-7         0.003363          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43         1E-7        0.004062          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.004028          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-8         0.003838          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-8         0.006478          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-8          0.01036          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-8          0.01054          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-8         0.003932          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-8         0.004727          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44         1E-8        0.006271          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.01195          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-8         0.003806          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-8         0.003716          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-8         0.005917          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-8         0.003279          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-8         0.002752          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-8         0.004264          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45         1E-8        0.005097          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.01009          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-8         0.002417          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-8         0.002604          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-8           0.0023          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-8          0.01406          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-8          0.01168          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-8         0.005094          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46         1E-8        0.006892          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.006505          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-8         0.002706          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-8          0.00292          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-8          0.01205          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-8         0.003124          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-8         0.002203          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-8         0.002111          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47         1E-8        0.004517          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.002649          0     0.2877     0.04\n",
      "NOTE:      1    64     1E-8         0.004203          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-8         0.002135          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-8         0.004641          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-8         0.002136          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-8         0.004556          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-8          0.01086          0     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48         1E-8        0.004455          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.002324          0     0.2877     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64     1E-8         0.007725          0     0.2877     0.03\n",
      "NOTE:      2    64     1E-8         0.006003          0     0.2877     0.03\n",
      "NOTE:      3    64     1E-8         0.003889          0     0.2877     0.03\n",
      "NOTE:      4    64     1E-8         0.002642          0     0.2877     0.03\n",
      "NOTE:      5    64     1E-8         0.004155          0     0.2877     0.03\n",
      "NOTE:      6    64     1E-8          0.09598    0.04688     0.2877     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49         1E-8         0.01753   0.006696     0.21\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      10.50 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_kxv6go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.289747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.015118</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.289025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>61</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.022175</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.288641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.014250</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.288298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.029018</td>\n",
       "      <td>0.288102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.287992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.017813</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.287921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>66</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.287867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>67</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>68</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>69</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>71</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>72</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>73</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>74</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>75</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>76</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.006952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>77</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.006704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>79</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>80</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>81</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.011061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>82</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>83</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>84</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>85</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>86</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>87</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>88</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>89</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>90</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>91</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.009574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>92</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>93</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>94</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>95</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>96</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.003447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>97</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.004877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>98</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>99</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>101</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>102</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.006892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>103</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>104</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>105</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.017531</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.287716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>Model_kxV6go_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_kxV6go_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 12.8s</span> &#183; <span class=\"cas-user\">user 20.7s</span> &#183; <span class=\"cas-sys\">sys 4.49s</span> &#183; <span class=\"cas-memory\">mem 130MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_kxv6go\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0      56  1.000000e-02  0.004123  0.000000  0.290244\n",
       " 1      57  1.000000e-02  0.003149  0.000000  0.290047\n",
       " 2      58  1.000000e-02  0.008805  0.002232  0.289747\n",
       " 3      59  1.000000e-02  0.003606  0.000000  0.289398\n",
       " 4      60  1.000000e-02  0.015118  0.004464  0.289025\n",
       " 5      61  1.000000e-02  0.022175  0.011161  0.288641\n",
       " 6      62  1.000000e-03  0.014250  0.006696  0.288298\n",
       " 7      63  1.000000e-03  0.048100  0.029018  0.288102\n",
       " 8      64  1.000000e-03  0.010400  0.004464  0.287992\n",
       " 9      65  1.000000e-03  0.017813  0.008929  0.287921\n",
       " 10     66  1.000000e-03  0.008511  0.002232  0.287867\n",
       " 11     67  1.000000e-03  0.005366  0.000000  0.287822\n",
       " 12     68  1.000000e-04  0.005553  0.000000  0.287784\n",
       " 13     69  1.000000e-04  0.003788  0.000000  0.287761\n",
       " 14     70  1.000000e-04  0.003429  0.000000  0.287748\n",
       " 15     71  1.000000e-04  0.005027  0.000000  0.287740\n",
       " 16     72  1.000000e-04  0.004067  0.000000  0.287734\n",
       " 17     73  1.000000e-04  0.005786  0.000000  0.287729\n",
       " 18     74  1.000000e-05  0.005732  0.000000  0.287725\n",
       " 19     75  1.000000e-05  0.009606  0.000000  0.287723\n",
       " 20     76  1.000000e-05  0.006952  0.000000  0.287721\n",
       " 21     77  1.000000e-05  0.006704  0.000000  0.287720\n",
       " 22     78  1.000000e-05  0.002959  0.000000  0.287720\n",
       " 23     79  1.000000e-05  0.005134  0.000000  0.287719\n",
       " 24     80  1.000000e-05  0.007016  0.000000  0.287719\n",
       " 25     81  1.000000e-05  0.011061  0.000000  0.287718\n",
       " 26     82  1.000000e-05  0.002919  0.000000  0.287718\n",
       " 27     83  1.000000e-05  0.004939  0.000000  0.287718\n",
       " 28     84  1.000000e-05  0.003919  0.000000  0.287717\n",
       " 29     85  1.000000e-05  0.003544  0.000000  0.287717\n",
       " 30     86  1.000000e-05  0.006301  0.000000  0.287716\n",
       " 31     87  1.000000e-06  0.004276  0.000000  0.287716\n",
       " 32     88  1.000000e-06  0.005734  0.000000  0.287716\n",
       " 33     89  1.000000e-06  0.002786  0.000000  0.287716\n",
       " 34     90  1.000000e-06  0.003146  0.000000  0.287716\n",
       " 35     91  1.000000e-06  0.009574  0.000000  0.287716\n",
       " 36     92  1.000000e-06  0.004625  0.000000  0.287716\n",
       " 37     93  1.000000e-06  0.007505  0.000000  0.287716\n",
       " 38     94  1.000000e-07  0.003659  0.000000  0.287716\n",
       " 39     95  1.000000e-07  0.003538  0.000000  0.287716\n",
       " 40     96  1.000000e-07  0.003447  0.000000  0.287716\n",
       " 41     97  1.000000e-07  0.004877  0.000000  0.287716\n",
       " 42     98  1.000000e-07  0.003896  0.000000  0.287716\n",
       " 43     99  1.000000e-07  0.004062  0.000000  0.287716\n",
       " 44    100  1.000000e-08  0.006271  0.000000  0.287716\n",
       " 45    101  1.000000e-08  0.005097  0.000000  0.287716\n",
       " 46    102  1.000000e-08  0.006892  0.000000  0.287716\n",
       " 47    103  1.000000e-08  0.004517  0.000000  0.287716\n",
       " 48    104  1.000000e-08  0.004455  0.000000  0.287716\n",
       " 49    105  1.000000e-08  0.017531  0.006696  0.287716\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  Model_kxV6go_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_kxV6go_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 12.8s, user: 20.7s, sys: 4.49s, mem: 130mb"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, n_threads=4, record_seed=13309, optimizer=optimizer,\n",
    "                      gpu=gpu, log_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize Learning Rate Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a flexible approach to define your learning rate policy using FCMP function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00734s</span> &#183; <span class=\"cas-user\">user 0.00518s</span> &#183; <span class=\"cas-sys\">sys 0.00204s</span> &#183; <span class=\"cas-memory\">mem 2.51MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.00734s, user: 0.00518s, sys: 0.00204s, mem: 2.51mb"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cool_down_iters = 5\n",
    "patience = 1\n",
    "sess.addRoutines(\n",
    "            routineCode = '''\n",
    "                        function reduce_lr_on_plateau(rate, initRate, gamma, loss[*]);\n",
    "                            len = dim(loss);\n",
    "                            temp_rate = initRate;\n",
    "                            cool_down_counter = {0};\n",
    "                            best = loss[1];\n",
    "                            do i=1 to len;\n",
    "                    \n",
    "                                if loss[i] < best then do;\n",
    "                                    best = loss[i];\n",
    "                                    bad_epoch = 0;\n",
    "                                end;\n",
    "                                else bad_epoch = bad_epoch + 1;\n",
    "                    \n",
    "                                if cool_down_counter > 0 then do;\n",
    "                                    cool_down_counter = cool_down_counter - 1;\n",
    "                                    bad_epoch = 0;\n",
    "                                end;\n",
    "                    \n",
    "                                if bad_epoch > {1} then do;\n",
    "                                    temp_rate = temp_rate * gamma;\n",
    "                                    cool_down_counter = {0};\n",
    "                                    bad_epoch = 0;\n",
    "                                end;\n",
    "                            end;\n",
    "                            rate = temp_rate;\n",
    "                            put rate=;\n",
    "                            return(rate);\n",
    "                        endsub;\n",
    "                        '''.format(cool_down_iters, patience),\n",
    "            package = 'pkg',\n",
    "            funcTable = dict(name = 'reduce_lr_on_plateau', replace = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input your FCMP function name in fcmp_learning_rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, learning_rate_policy are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = FCMPLR(conn=sess, fcmp_learning_rate='reduce_lr_on_plateau',\n",
    "                      learning_rate = 0.01, gamma = 0.1)\n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        clip_grad_max = 100, clip_grad_min = -100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=50, reg_l2=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE: Using dlgrd010.unx.sas.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.01 (s).\n",
      "NOTE:  Initializing each layer cost       1.84 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.01392          0     0.2877     0.20\n",
      "NOTE:      1    64     0.01         0.001961          0     0.2877     0.03\n",
      "NOTE:      2    64     0.01          0.01443          0     0.2877     0.03\n",
      "NOTE:      3    64     0.01         0.002757          0     0.2877     0.03\n",
      "NOTE:      4    64     0.01           0.0104          0     0.2877     0.03\n",
      "NOTE:      5    64     0.01          0.01012          0     0.2876     0.03\n",
      "NOTE:      6    64     0.01         0.002718          0     0.2876     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0          0.01        0.008045          0     0.37\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002839          0     0.2876     0.04\n",
      "NOTE:      1    64     0.01          0.00545          0     0.2876     0.03\n",
      "NOTE:      2    64     0.01         0.003771          0     0.2875     0.03\n",
      "NOTE:      3    64     0.01         0.001888          0     0.2875     0.03\n",
      "NOTE:      4    64     0.01         0.002298          0     0.2874     0.03\n",
      "NOTE:      5    64     0.01         0.002752          0     0.2874     0.03\n",
      "NOTE:      6    64     0.01         0.001997          0     0.2874     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1          0.01           0.003          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.001889          0     0.2873     0.04\n",
      "NOTE:      1    64     0.01          0.01008          0     0.2873     0.03\n",
      "NOTE:      2    64     0.01         0.002386          0     0.2872     0.03\n",
      "NOTE:      3    64     0.01         0.004552          0     0.2872     0.03\n",
      "NOTE:      4    64     0.01         0.001929          0     0.2871     0.03\n",
      "NOTE:      5    64     0.01         0.001643          0     0.2871     0.03\n",
      "NOTE:      6    64     0.01         0.002295          0      0.287     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          0.01         0.00354          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002183          0      0.287     0.04\n",
      "NOTE:      1    64     0.01         0.001528          0     0.2869     0.03\n",
      "NOTE:      2    64     0.01          0.00376          0     0.2869     0.03\n",
      "NOTE:      3    64     0.01         0.001656          0     0.2868     0.03\n",
      "NOTE:      4    64     0.01          0.00194          0     0.2868     0.03\n",
      "NOTE:      5    64     0.01         0.001579          0     0.2867     0.03\n",
      "NOTE:      6    64     0.01          0.01133          0     0.2867     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3          0.01        0.003425          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.005577          0     0.2866     0.04\n",
      "NOTE:      1    64     0.01         0.001718          0     0.2866     0.03\n",
      "NOTE:      2    64     0.01         0.001807          0     0.2865     0.03\n",
      "NOTE:      3    64     0.01          0.01089          0     0.2865     0.03\n",
      "NOTE:      4    64     0.01         0.003681          0     0.2864     0.03\n",
      "NOTE:      5    64     0.01         0.003885          0     0.2864     0.03\n",
      "NOTE:      6    64     0.01         0.002353          0     0.2863     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4          0.01        0.004272          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002529          0     0.2863     0.04\n",
      "NOTE:      1    64     0.01         0.001768          0     0.2862     0.03\n",
      "NOTE:      2    64     0.01          0.00311          0     0.2861     0.03\n",
      "NOTE:      3    64     0.01         0.003504          0     0.2861     0.03\n",
      "NOTE:      4    64     0.01         0.001523          0      0.286     0.03\n",
      "NOTE:      5    64     0.01         0.004346          0      0.286     0.03\n",
      "NOTE:      6    64     0.01         0.008696          0     0.2859     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5          0.01        0.003639          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.001364          0     0.2859     0.04\n",
      "NOTE:      1    64     0.01         0.004707          0     0.2858     0.03\n",
      "NOTE:      2    64     0.01         0.001342          0     0.2858     0.03\n",
      "NOTE:      3    64     0.01         0.002672          0     0.2857     0.03\n",
      "NOTE:      4    64     0.01         0.001558          0     0.2856     0.03\n",
      "NOTE:      5    64     0.01          0.01188          0     0.2856     0.03\n",
      "NOTE:      6    64     0.01         0.005005          0     0.2855     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6          0.01        0.004076          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001255          0     0.2855     0.04\n",
      "NOTE:      1    64    0.001          0.01208          0     0.2854     0.03\n",
      "NOTE:      2    64    0.001         0.002041          0     0.2854     0.03\n",
      "NOTE:      3    64    0.001         0.005653          0     0.2853     0.03\n",
      "NOTE:      4    64    0.001         0.002913          0     0.2853     0.03\n",
      "NOTE:      5    64    0.001         0.001448          0     0.2853     0.03\n",
      "NOTE:      6    64    0.001         0.001564          0     0.2852     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.001        0.003851          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.002554          0     0.2852     0.04\n",
      "NOTE:      1    64    0.001         0.001695          0     0.2852     0.03\n",
      "NOTE:      2    64    0.001         0.009868          0     0.2852     0.03\n",
      "NOTE:      3    64    0.001         0.004406          0     0.2851     0.03\n",
      "NOTE:      4    64    0.001         0.003942          0     0.2851     0.03\n",
      "NOTE:      5    64    0.001         0.007982          0     0.2851     0.03\n",
      "NOTE:      6    64    0.001          0.00161          0     0.2851     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8         0.001         0.00458          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.00286          0     0.2851     0.04\n",
      "NOTE:      1    64    0.001         0.002364          0      0.285     0.03\n",
      "NOTE:      2    64    0.001         0.002023          0      0.285     0.03\n",
      "NOTE:      3    64    0.001         0.001362          0      0.285     0.03\n",
      "NOTE:      4    64    0.001         0.002762          0      0.285     0.03\n",
      "NOTE:      5    64    0.001         0.003128          0      0.285     0.03\n",
      "NOTE:      6    64    0.001         0.001789          0      0.285     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9         0.001        0.002327          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001416          0      0.285     0.04\n",
      "NOTE:      1    64    0.001         0.005813          0      0.285     0.03\n",
      "NOTE:      2    64    0.001         0.003293          0     0.2849     0.03\n",
      "NOTE:      3    64    0.001         0.001672          0     0.2849     0.03\n",
      "NOTE:      4    64    0.001          0.01881          0     0.2849     0.03\n",
      "NOTE:      5    64    0.001         0.001315          0     0.2849     0.03\n",
      "NOTE:      6    64    0.001         0.001747          0     0.2849     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10        0.001        0.004866          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.002485          0     0.2849     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64    0.001         0.003013          0     0.2849     0.03\n",
      "NOTE:      2    64    0.001         0.001283          0     0.2849     0.03\n",
      "NOTE:      3    64    0.001          0.00154          0     0.2849     0.03\n",
      "NOTE:      4    64    0.001         0.002292          0     0.2849     0.03\n",
      "NOTE:      5    64    0.001         0.004101          0     0.2849     0.03\n",
      "NOTE:      6    64    0.001         0.002939          0     0.2849     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.001        0.002522          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001389          0     0.2849     0.04\n",
      "NOTE:      1    64    0.001         0.001662          0     0.2848     0.03\n",
      "NOTE:      2    64    0.001         0.003256          0     0.2848     0.03\n",
      "NOTE:      3    64    0.001         0.002273          0     0.2848     0.03\n",
      "NOTE:      4    64    0.001         0.001886          0     0.2848     0.03\n",
      "NOTE:      5    64    0.001         0.001724          0     0.2848     0.03\n",
      "NOTE:      6    64    0.001         0.002624          0     0.2848     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12        0.001        0.002117          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.00158          0     0.2848     0.04\n",
      "NOTE:      1    64    0.001         0.002946          0     0.2848     0.03\n",
      "NOTE:      2    64    0.001          0.00946          0     0.2848     0.03\n",
      "NOTE:      3    64    0.001         0.001406          0     0.2848     0.03\n",
      "NOTE:      4    64    0.001         0.008179          0     0.2848     0.03\n",
      "NOTE:      5    64    0.001         0.002571          0     0.2848     0.03\n",
      "NOTE:      6    64    0.001         0.001508          0     0.2848     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13        0.001         0.00395          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001326          0     0.2848     0.04\n",
      "NOTE:      1    64    0.001         0.001609          0     0.2848     0.03\n",
      "NOTE:      2    64    0.001         0.001254          0     0.2848     0.03\n",
      "NOTE:      3    64    0.001         0.003602          0     0.2847     0.03\n",
      "NOTE:      4    64    0.001         0.001448          0     0.2847     0.03\n",
      "NOTE:      5    64    0.001          0.00835          0     0.2847     0.03\n",
      "NOTE:      6    64    0.001         0.002965          0     0.2847     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14        0.001        0.002936          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001409          0     0.2847     0.04\n",
      "NOTE:      1    64   0.0001         0.001228          0     0.2847     0.03\n",
      "NOTE:      2    64   0.0001         0.001826          0     0.2847     0.03\n",
      "NOTE:      3    64   0.0001         0.001496          0     0.2847     0.03\n",
      "NOTE:      4    64   0.0001          0.03389          0     0.2847     0.03\n",
      "NOTE:      5    64   0.0001         0.001672          0     0.2847     0.03\n",
      "NOTE:      6    64   0.0001         0.001503          0     0.2847     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15       0.0001        0.006147          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.002865          0     0.2847     0.04\n",
      "NOTE:      1    64   0.0001         0.003695          0     0.2847     0.03\n",
      "NOTE:      2    64   0.0001         0.002234          0     0.2847     0.03\n",
      "NOTE:      3    64   0.0001         0.003004          0     0.2847     0.03\n",
      "NOTE:      4    64   0.0001         0.001475          0     0.2847     0.03\n",
      "NOTE:      5    64   0.0001         0.002811          0     0.2847     0.03\n",
      "NOTE:      6    64   0.0001         0.002857          0     0.2847     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16       0.0001        0.002706          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.007944          0     0.2847     0.04\n",
      "NOTE:      1    64   0.0001         0.001822          0     0.2847     0.03\n",
      "NOTE:      2    64   0.0001         0.001269          0     0.2847     0.03\n",
      "NOTE:      3    64   0.0001          0.00239          0     0.2847     0.03\n",
      "NOTE:      4    64   0.0001         0.001249          0     0.2847     0.03\n",
      "NOTE:      5    64   0.0001         0.001508          0     0.2847     0.03\n",
      "NOTE:      6    64   0.0001         0.001364          0     0.2847     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0001        0.002507          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.003252          0     0.2847     0.04\n",
      "NOTE:      1    64   0.0001         0.002034          0     0.2847     0.03\n",
      "NOTE:      2    64   0.0001          0.00778          0     0.2847     0.03\n",
      "NOTE:      3    64   0.0001         0.004288          0     0.2847     0.03\n",
      "NOTE:      4    64   0.0001         0.001595          0     0.2847     0.03\n",
      "NOTE:      5    64   0.0001         0.002609          0     0.2847     0.03\n",
      "NOTE:      6    64   0.0001         0.001124          0     0.2847     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18       0.0001         0.00324          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001229          0     0.2847     0.04\n",
      "NOTE:      1    64   0.0001         0.003214          0     0.2847     0.03\n",
      "NOTE:      2    64   0.0001         0.007787          0     0.2847     0.03\n",
      "NOTE:      3    64   0.0001         0.003157          0     0.2847     0.03\n",
      "NOTE:      4    64   0.0001         0.001804          0     0.2847     0.03\n",
      "NOTE:      5    64   0.0001         0.001464          0     0.2847     0.03\n",
      "NOTE:      6    64   0.0001         0.001543          0     0.2847     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19       0.0001        0.002885          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001382          0     0.2847     0.04\n",
      "NOTE:      1    64   0.0001         0.008963          0     0.2847     0.03\n",
      "NOTE:      2    64   0.0001         0.002742          0     0.2847     0.03\n",
      "NOTE:      3    64   0.0001         0.002224          0     0.2847     0.03\n",
      "NOTE:      4    64   0.0001         0.001286          0     0.2847     0.03\n",
      "NOTE:      5    64   0.0001         0.001807          0     0.2847     0.03\n",
      "NOTE:      6    64   0.0001         0.002124          0     0.2847     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20       0.0001        0.002933          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001519          0     0.2847     0.04\n",
      "NOTE:      1    64   0.0001         0.001123          0     0.2847     0.03\n",
      "NOTE:      2    64   0.0001         0.002839          0     0.2847     0.03\n",
      "NOTE:      3    64   0.0001         0.001169          0     0.2847     0.03\n",
      "NOTE:      4    64   0.0001         0.001679          0     0.2847     0.03\n",
      "NOTE:      5    64   0.0001         0.001859          0     0.2847     0.03\n",
      "NOTE:      6    64   0.0001         0.009429          0     0.2847     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21       0.0001        0.002802          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.008069          0     0.2847     0.04\n",
      "NOTE:      1    64  0.00001         0.002683          0     0.2847     0.03\n",
      "NOTE:      2    64  0.00001         0.001239          0     0.2847     0.03\n",
      "NOTE:      3    64  0.00001         0.001198          0     0.2847     0.03\n",
      "NOTE:      4    64  0.00001         0.001465          0     0.2846     0.03\n",
      "NOTE:      5    64  0.00001         0.001288          0     0.2846     0.03\n",
      "NOTE:      6    64  0.00001         0.007715          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22         1E-5         0.00338          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.001416          0     0.2846     0.04\n",
      "NOTE:      1    64  0.00001         0.007779          0     0.2846     0.03\n",
      "NOTE:      2    64  0.00001         0.001944          0     0.2846     0.03\n",
      "NOTE:      3    64  0.00001         0.002789          0     0.2846     0.03\n",
      "NOTE:      4    64  0.00001         0.009352          0     0.2846     0.03\n",
      "NOTE:      5    64  0.00001         0.002686          0     0.2846     0.03\n",
      "NOTE:      6    64  0.00001         0.001187          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23         1E-5        0.003879          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.007765          0     0.2846     0.04\n",
      "NOTE:      1    64  0.00001         0.001445          0     0.2846     0.03\n",
      "NOTE:      2    64  0.00001         0.007844          0     0.2846     0.03\n",
      "NOTE:      3    64  0.00001         0.001309          0     0.2846     0.03\n",
      "NOTE:      4    64  0.00001         0.002994          0     0.2846     0.03\n",
      "NOTE:      5    64  0.00001         0.001709          0     0.2846     0.03\n",
      "NOTE:      6    64  0.00001         0.007817          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24         1E-5        0.004412          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.001323          0     0.2846     0.04\n",
      "NOTE:      1    64  0.00001         0.001156          0     0.2846     0.03\n",
      "NOTE:      2    64  0.00001         0.001424          0     0.2846     0.03\n",
      "NOTE:      3    64  0.00001         0.001432          0     0.2846     0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      4    64  0.00001         0.001311          0     0.2846     0.03\n",
      "NOTE:      5    64  0.00001         0.001322          0     0.2846     0.03\n",
      "NOTE:      6    64  0.00001         0.002478          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25         1E-5        0.001492          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.001229          0     0.2846     0.04\n",
      "NOTE:      1    64  0.00001         0.002922          0     0.2846     0.03\n",
      "NOTE:      2    64  0.00001          0.00854          0     0.2846     0.03\n",
      "NOTE:      3    64  0.00001         0.002472          0     0.2846     0.03\n",
      "NOTE:      4    64  0.00001         0.001563          0     0.2846     0.03\n",
      "NOTE:      5    64  0.00001         0.001332          0     0.2846     0.03\n",
      "NOTE:      6    64  0.00001         0.002177          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26         1E-5        0.002891          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.001201          0     0.2846     0.04\n",
      "NOTE:      1    64  0.00001         0.008024          0     0.2846     0.04\n",
      "NOTE:      2    64  0.00001         0.001611          0     0.2846     0.03\n",
      "NOTE:      3    64  0.00001         0.001608          0     0.2846     0.03\n",
      "NOTE:      4    64  0.00001         0.001572          0     0.2846     0.03\n",
      "NOTE:      5    64  0.00001         0.008142          0     0.2846     0.03\n",
      "NOTE:      6    64  0.00001         0.001236          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27         1E-5        0.003342          0     0.22\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.004209          0     0.2846     0.04\n",
      "NOTE:      1    64  0.00001         0.001659          0     0.2846     0.03\n",
      "NOTE:      2    64  0.00001         0.001528          0     0.2846     0.03\n",
      "NOTE:      3    64  0.00001         0.001493          0     0.2846     0.03\n",
      "NOTE:      4    64  0.00001         0.003259          0     0.2846     0.03\n",
      "NOTE:      5    64  0.00001         0.001421          0     0.2846     0.03\n",
      "NOTE:      6    64  0.00001         0.001248          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28         1E-5        0.002117          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.001456          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-6         0.002773          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-6         0.007988          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-6         0.007813          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-6         0.001415          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-6         0.001342          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-6         0.001921          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29         1E-6         0.00353          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.003087          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-6         0.007844          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-6         0.008168          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-6         0.001268          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-6         0.001339          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-6         0.001662          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-6         0.004126          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30         1E-6        0.003928          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002634          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-6         0.001627          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-6         0.009535          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-6         0.002054          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-6         0.004115          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-6         0.001474          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-6         0.001281          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31         1E-6        0.003245          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.001822          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-6         0.001401          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-6         0.003047          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-6          0.00119          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-6         0.007602          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-6         0.002967          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-6         0.002908          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32         1E-6        0.002991          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.001632          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-6         0.001638          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-6          0.00168          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-6         0.007766          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-6         0.001237          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-6         0.002872          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-6         0.001631          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33         1E-6        0.002637          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.001387          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-6         0.002936          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-6         0.009533          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-6         0.001261          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-6         0.001369          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-6         0.001212          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-6         0.001771          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34         1E-6        0.002781          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002682          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-6         0.009523          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-6         0.001842          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-6         0.001338          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-6         0.002083          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-6         0.009499          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-6         0.004977          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35         1E-6        0.004563          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.002855          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-7         0.002413          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-7         0.001867          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-7         0.001301          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-7         0.001457          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-7         0.001613          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-7         0.007784          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36         1E-7        0.002756          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.003005          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-7         0.009555          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-7         0.001351          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-7         0.002775          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-7         0.001613          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-7         0.007718          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-7          0.00145          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37         1E-7        0.003924          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.001312          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-7         0.001633          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-7         0.003263          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-7         0.001282          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-7         0.001358          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-7         0.001443          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-7         0.002808          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38         1E-7        0.001871          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.002776          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-7         0.002862          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-7          0.00137          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-7         0.002943          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-7         0.004293          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-7         0.001745          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-7         0.001773          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39         1E-7        0.002537          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.002902          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-7         0.001664          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-7         0.001309          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-7         0.001414          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-7         0.001447          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-7          0.00837          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-7         0.001587          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40         1E-7        0.002671          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.001475          0     0.2846     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64     1E-7         0.003155          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-7         0.002503          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-7         0.001267          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-7         0.002444          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-7         0.001434          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-7         0.001574          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41         1E-7        0.001979          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.003793          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-7         0.001905          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-7         0.001339          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-7         0.002041          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-7         0.001599          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-7         0.002754          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-7         0.005198          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42         1E-7        0.002661          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.00143          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-8         0.001339          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-8         0.001408          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-8         0.001267          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-8         0.001232          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-8         0.004511          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-8         0.002119          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43         1E-8        0.001901          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.001381          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-8         0.002723          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-8         0.001389          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-8         0.001323          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-8         0.001741          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-8         0.001788          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-8         0.002818          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44         1E-8        0.001881          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.009184          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-8         0.001775          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-8         0.002655          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-8         0.001178          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-8         0.002581          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-8         0.005875          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-8          0.01439          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45         1E-8        0.005377          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.00175          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-8          0.00428          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-8         0.001639          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-8         0.007693          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-8         0.001906          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-8         0.001524          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-8         0.001704          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46         1E-8        0.002928          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.001591          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-8          0.00793          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-8         0.001497          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-8          0.00155          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-8         0.001413          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-8         0.001887          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-8         0.009961          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47         1E-8         0.00369          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.001305          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-8         0.007674          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-8         0.003694          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-8         0.001382          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-8         0.009442          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-8         0.001848          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-8         0.001869          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48         1E-8        0.003888          0     0.21\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.009477          0     0.2846     0.04\n",
      "NOTE:      1    64     1E-8         0.003282          0     0.2846     0.03\n",
      "NOTE:      2    64     1E-8         0.001416          0     0.2846     0.03\n",
      "NOTE:      3    64     1E-8         0.001781          0     0.2846     0.03\n",
      "NOTE:      4    64     1E-8         0.001426          0     0.2846     0.03\n",
      "NOTE:      5    64     1E-8         0.001766          0     0.2846     0.03\n",
      "NOTE:      6    64     1E-8         0.002704          0     0.2846     0.03\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49         1E-8        0.003122          0     0.21\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      10.51 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_kxv6go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.008045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.287677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.287483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.287187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>111</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>112</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>114</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>115</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>116</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>117</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>118</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>119</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>120</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>121</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.006147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>122</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>123</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>124</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>125</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>126</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>127</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>128</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>129</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>130</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>131</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>132</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>133</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>134</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>135</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>136</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>137</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>138</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>139</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>140</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>141</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>142</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>143</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>144</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>145</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>146</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>147</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>148</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>149</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>150</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>151</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>152</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>153</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>154</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>155</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(weshiz)</td>\n",
       "      <td>Model_kxV6go_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_kxV6go_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 12.8s</span> &#183; <span class=\"cas-user\">user 20.5s</span> &#183; <span class=\"cas-sys\">sys 4.47s</span> &#183; <span class=\"cas-memory\">mem 130MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_kxv6go\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0     106  1.000000e-02  0.008045       0.0  0.287677\n",
       " 1     107  1.000000e-02  0.003000       0.0  0.287483\n",
       " 2     108  1.000000e-02  0.003540       0.0  0.287187\n",
       " 3     109  1.000000e-02  0.003425       0.0  0.286841\n",
       " 4     110  1.000000e-02  0.004272       0.0  0.286471\n",
       " 5     111  1.000000e-02  0.003639       0.0  0.286090\n",
       " 6     112  1.000000e-02  0.004076       0.0  0.285705\n",
       " 7     113  1.000000e-03  0.003851       0.0  0.285352\n",
       " 8     114  1.000000e-03  0.004580       0.0  0.285139\n",
       " 9     115  1.000000e-03  0.002327       0.0  0.285017\n",
       " 10    116  1.000000e-03  0.004866       0.0  0.284938\n",
       " 11    117  1.000000e-03  0.002522       0.0  0.284881\n",
       " 12    118  1.000000e-03  0.002117       0.0  0.284833\n",
       " 13    119  1.000000e-03  0.003950       0.0  0.284789\n",
       " 14    120  1.000000e-03  0.002936       0.0  0.284748\n",
       " 15    121  1.000000e-04  0.006147       0.0  0.284712\n",
       " 16    122  1.000000e-04  0.002706       0.0  0.284690\n",
       " 17    123  1.000000e-04  0.002507       0.0  0.284677\n",
       " 18    124  1.000000e-04  0.003240       0.0  0.284669\n",
       " 19    125  1.000000e-04  0.002885       0.0  0.284663\n",
       " 20    126  1.000000e-04  0.002933       0.0  0.284659\n",
       " 21    127  1.000000e-04  0.002802       0.0  0.284654\n",
       " 22    128  1.000000e-05  0.003380       0.0  0.284650\n",
       " 23    129  1.000000e-05  0.003879       0.0  0.284648\n",
       " 24    130  1.000000e-05  0.004412       0.0  0.284647\n",
       " 25    131  1.000000e-05  0.001492       0.0  0.284646\n",
       " 26    132  1.000000e-05  0.002891       0.0  0.284646\n",
       " 27    133  1.000000e-05  0.003342       0.0  0.284645\n",
       " 28    134  1.000000e-05  0.002117       0.0  0.284645\n",
       " 29    135  1.000000e-06  0.003530       0.0  0.284644\n",
       " 30    136  1.000000e-06  0.003928       0.0  0.284644\n",
       " 31    137  1.000000e-06  0.003245       0.0  0.284644\n",
       " 32    138  1.000000e-06  0.002991       0.0  0.284644\n",
       " 33    139  1.000000e-06  0.002637       0.0  0.284644\n",
       " 34    140  1.000000e-06  0.002781       0.0  0.284644\n",
       " 35    141  1.000000e-06  0.004563       0.0  0.284644\n",
       " 36    142  1.000000e-07  0.002756       0.0  0.284644\n",
       " 37    143  1.000000e-07  0.003924       0.0  0.284644\n",
       " 38    144  1.000000e-07  0.001871       0.0  0.284644\n",
       " 39    145  1.000000e-07  0.002537       0.0  0.284644\n",
       " 40    146  1.000000e-07  0.002671       0.0  0.284644\n",
       " 41    147  1.000000e-07  0.001979       0.0  0.284644\n",
       " 42    148  1.000000e-07  0.002661       0.0  0.284644\n",
       " 43    149  1.000000e-08  0.001901       0.0  0.284644\n",
       " 44    150  1.000000e-08  0.001881       0.0  0.284644\n",
       " 45    151  1.000000e-08  0.005377       0.0  0.284644\n",
       " 46    152  1.000000e-08  0.002928       0.0  0.284644\n",
       " 47    153  1.000000e-08  0.003690       0.0  0.284644\n",
       " 48    154  1.000000e-08  0.003888       0.0  0.284644\n",
       " 49    155  1.000000e-08  0.003122       0.0  0.284644\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(weshiz)  Model_kxV6go_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_kxV6go_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 12.8s, user: 20.5s, sys: 4.47s, mem: 130mb"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, n_threads=4, record_seed=13309, optimizer=optimizer,\n",
    "                      gpu=gpu, log_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000293s</span> &#183; <span class=\"cas-user\">user 0.000147s</span> &#183; <span class=\"cas-sys\">sys 0.000128s</span> &#183; <span class=\"cas-memory\">mem 0.212MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.000293s, user: 0.000147s, sys: 0.000128s, mem: 0.212mb"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.endsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
