{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using SAS DLPy to Convert Speech to Text</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses SAS DLPy to convert speech to text. \n",
    "\n",
    "The example begins with configuring the environment. After that, the SpeechToText object is created and the pretrained acoustic model and language model are loaded, which followed by applying the models to do speech-to-text.\n",
    "\n",
    "The example assumes that you're using the pretrained acoustic and language models published for SAS Viya 3.4, which you can download them from here: https://support.sas.com/documentation/prod-p/vdmml/zip/speech_19w21.zip. The example also assumes that the inputs will be audio files in WAV format with 8-bit, 16-bit, 24-bit or 32-bit sampling encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Configure the Environment<a name=\"configureIt\"></a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing `SWAT`. SWAT is a Python interface to SAS CAS. For more information about starting a CAS session with the SWAT package, see https://sassoftware.github.io/python-swat/getting-started.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swat import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import DLPy `speech`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After configuring your environment and loading required libraries, connect to your CAS server. You will need a host name and port number for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = CAS(cashost, casport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 'caslib_speech' is now the active caslib.\n",
      "NOTE: Cloud Analytic Services added the caslib 'caslib_speech'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; CASLibInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Type\">Type</th>\n",
       "      <th title=\"Description\">Description</th>\n",
       "      <th title=\"Path\">Path</th>\n",
       "      <th title=\"Definition\">Definition</th>\n",
       "      <th title=\"Subdirs\">Subdirs</th>\n",
       "      <th title=\"Local\">Local</th>\n",
       "      <th title=\"Active\">Active</th>\n",
       "      <th title=\"Personal\">Personal</th>\n",
       "      <th title=\"Hidden\">Hidden</th>\n",
       "      <th title=\"Transient\">Transient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caslib_speech</td>\n",
       "      <td>PATH</td>\n",
       "      <td></td>\n",
       "      <td>/dept/cas/xixche/data/</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00161s</span> &#183; <span class=\"cas-user\">user 0.000425s</span> &#183; <span class=\"cas-sys\">sys 7.9e-05s</span> &#183; <span class=\"cas-memory\">mem 0.711MB</span></small></p>"
      ],
      "text/plain": [
       "[CASLibInfo]\n",
       "\n",
       "             Name  Type Description                    Path Definition  \\\n",
       " 0  caslib_speech  PATH              /dept/cas/xixche/data/              \n",
       " \n",
       "    Subdirs  Local  Active  Personal  Hidden  Transient  \n",
       " 0      1.0    1.0     1.0       0.0     0.0        0.0  \n",
       "\n",
       "+ Elapsed: 0.00161s, user: 0.000425s, sys: 7.9e-05s, mem: 0.711mb"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.table.addcaslib(datasource = {\"srcType\": \"path\"},\n",
    "                  name = \"caslib_speech\",\n",
    "                  path = \"/dept/cas/xixche/data\",\n",
    "                  subdirectories = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows caslib `caslib_speech` is created. The folder referenced in the `Path` column contains the acoustic model and language model files downloaded. This step is to prepare for accessing some data in the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a SpeechToText Object</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step creates a `speech.SpeechToText` object with the current CAS connection. \n",
    "\n",
    "When dealing with an input audio lasts quite long, we may need to segment the long audio into several shorter pieces first. We will save the segmented audio data somewhere on your disk temporarily and remove them once the API completes running. The location to store the audio files must be accessible by both the CAS server and the Python client. \n",
    "\n",
    "Therefore, we will have two paths to store the files: `data_path_after_caslib` and `local_path`. They actually point to the same location. The value of `data_path_after_caslib` is determined by the CAS server OS and should be relative to `caslib_data`, while `local_path` is determined by the Python client OS and can be either relative or absolute.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spch2txt = speech.SpeechToText(s, \n",
    "                               data_path_after_caslib             = \"tmp/\", \n",
    "                               local_path                         = \"\\\\\\\\sashq\\\\root\\\\dept\\\\cas\\\\xixche\\\\data\\\\tmp\", \n",
    "                               data_caslib                        = \"caslib_speech\", \n",
    "                               acoustic_model_path                = \"acoustic_model_cpu.sashdat\",\n",
    "                               acoustic_model_caslib              = \"caslib_speech\",\n",
    "                               acoustic_model_weights_path        = \"acoustic_model_cpu_weights.sashdat\",\n",
    "                               acoustic_model_weights_caslib      = \"caslib_speech\",\n",
    "                               acoustic_model_weights_attr_path   = \"acoustic_model_cpu_weights_attr.sashdat\",\n",
    "                               acoustic_model_weights_attr_caslib = \"caslib_speech\", \n",
    "                               language_model_path                = \"language_model.sashdat\",\n",
    "                               language_model_caslib              = \"caslib_speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load our pretrained models at the initialization step as shown above. Or you can choose to create the SpeechToText object first, and then load the models one by one by using the methods `load_acoustic_model` and `load_language_model` from `dlpy.speech`. If there is an acoustic or language model loaded already, the method will replace the old model by the new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convet Speech To Text</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the `speech.transcribe` method to convert speech to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW FEDERAL RULES REQUIRE THRIFTS TO GRADUALLY RAISE THAT LEVEL DEPENDING ON THE PROFITABILITY OF THE INDUSTRY'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spch2txt.transcribe(\"sample_1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THEY CAN BE HILARIOUSLY FUNNY AS WELL AS HIGHLY EFFECTIVE TOOLS FOR SATIRE'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spch2txt.transcribe(\"sample_2.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
